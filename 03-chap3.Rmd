# Methods {#methods-sec}

We now move into a thorough description of all of the methods employed in this thesis. In subsections \@ref(notation)-\@ref(specificmods) the necessary notation and model structure are introduced. While the rest of the subsections intermittently mention the Frequentist model, they are primarily focused on the Bayesian models. In particular, \@ref(twofits) describes how the Bayesian models are fit and \@ref(bayespred) describes what prediction with a Bayesian model looks like.


## Notation {#notation}

Let $U$ denote a finite population with $N$ elements. $U$ is broken into $J$ groups $U_j$, $j = 1, 2, ..., J$, where each group $U_j$ is defined as having $n_j$ sample observations. Let $p = 1, ..., P$ index the covariates. Each sample observation, $i$ in group $j$ has auxiliary information $x_{ij}^p$ for covariate $p$, response value $y_{ij}$, and indicator for being non-zero $z_{ij}$.

$$
z_{ij} =
\begin{cases}
1 & \text{if}\ \ y_{ij} \ne 0 \\
0 & \text{if} \ \ y_{ij} = 0
\end{cases}
$$


## Model Structure Motivation

We now introduce the modeling technique that will be the main focus of the thesis. Wonderfully, there is some real mathematical backing for why we might build this model in the way that we do, and I think that looking at the steps draws out a lot of helpful intuition.

Let $Y$ represent the response variable and $X$ represent the covariates. We typically write $\mathbb{E}[Y \ | \ X]$ to denote the expected value of our response variable conditional on it's covariates. Since the separation of the response into values that are zero and those that are not is a finite partition, we can leverage the law of iterated expectation to expand our model structure:

\begin{equation}
\begin{split}
    E[ Y  \ | \ X = x] &= \underbrace{E[Y \ | \ X = x, Y = 0]}_{= \ 0}P(Y = 0 \ | \ X = x) + \\
    & \ \ \ \ E[Y \ | \ X = x, \ Y > 0]P(Y> 0 \ | \ X = x) \\
    &= E[Y \ | \ X = x, \ Y > 0]P(Y > 0 \ | \ X = x) 
\end{split}
(\#eq:intuition)
\end{equation}


Out of this equation comes a wonderful intuition for what our new modeling process will look like. What we end up with is something that is somewhat meta in that it doesn't tell us what the exact model will be, but rather it tells us what the structure of our model should look like. What we do know is that we should have one model that predicts our non-zero response using our covariates $E[Y \ | \ X = x, \ Y>0]$, and another that predicts whether our response is non-zero or not, again using the covariates $P(Y > 0 \ | \ X = x)$. Think back to the Introduction where we introduced the model as being:

$$
\text{Final Prediction} = \bigg(\text{Classification Model Output}\bigg) \times \bigg(\text{Regression Model Output}\bigg) 
$$

all we've done above is provided a formal theoretical backing for this structure and strategy.

Importantly, while Equation \@ref(eq:intuition) describes the broader two-part model strategy that will be employed by both the Frequentist and Bayesian versions of the model, the formulation can't technically be extended into the context of prediction for the Bayesian model. Equation \@ref(eq:intuition) does describe how prediction functions in the Frequentist case, but as we will get to later in the Bayesian setting everything is described by a probability distribution and so we aren't working with expected values anymore. That being said, this is just a small technicality that shouldn't take away from the really nice larger intuition that Equation \@ref(eq:intuition) holds for both models.

## Specific Models {#specificmods}

Although we could model these two parts however we wanted to, we will use a logistic regression model for the classification model and a generalized linear regression model (that is non-logistic) for the regression model. Moreover, because the data we will be working with has a clustered structure we attempt to capture that by including group-level random effects in both models. The precise models that we will be evaluating will be as follows.

We fit a generalized linear model with random intercepts to the **non-zero** portion of the data (the $*$ helps differentiate this model from our final model). The linear predictor is specified as follows

$$
\mu_{ij} = \mathbf{x}_{ij}^T\boldsymbol{\beta} + u_j  \qquad \text{where} \qquad u_j \sim \mathcal{N}(0, \sigma_{u}^2)
$$

With a link function $g^{-1}$ and a probability distribution for the response, we get the final model as

$$
E\big[y^*_{ij} \ | \ x\big] = g^{-1}(\mu_{ij})
$$

To spoil the surprise, the two GLMs that we will look at will be one with identity link and Normal distribution (i.e normal linear regression), and one with the log link and a Gamma distribution. That being said, we keep the notation broad at this point so as to emphasize the fact that this portion of the model aims to capture the structure of the non-zero response, and even though we try to model it using various distributions, this is still the main goal.

Here, $\mathbf{x}_{ij}^T = (x^1_{ij}, ..., x^P_{ij})$ is a $P\times 1$ vector of covariates, $\boldsymbol{\beta}$ is a $1\times P$ vector of fixed effects, and $u_j$ is the random effect associated with group $j$. Finally, $\sigma^2_{u}$ is the between group variance parameter. 

Next we fit a logistic regression random intercepts model to the full data set

$$
P(z_{ij} = 1) = p_{ij} = \frac{1}{1 + e^{-(\mathbf{x}_{ij}^T\boldsymbol{\gamma} + v_j)}} \qquad \text{where} \qquad v_j \sim \mathcal{N}(0, \sigma_{v}^2)
$$

Here $\boldsymbol{\gamma}$ is a $1\times P$ vector of fixed effects and $v_j$ is the random effect associated with group $j$. Again, $\sigma^2_{v}$ is the between group variance parameter.

We will now quickly go over how these models are fit in a Frequentist frame before going in depth into how they are fit in a Bayesian frame.

## Model Fitting: Two Ways {#twofits}

### Frequentist

As it is not the focus of this thesis, we will not go in depth into how these models are fit in a Frequentist setting. Still, it's important to at least provide a brief summary of how it is most often done. 

In most cases (and in particular in most statistical software), Frequentist regression models are fit using something called Maximum Likelihood Estimation (MLE). Very broadly, MLE functions by first assuming that the observed data was sampled from some distribution. Out of that assumption we get a likelihood function $p(\text{data} \ | \ \text{parameters})$. And finally, as the name suggests, we choose parameter values that maximize the likelihood of the observed data given that parameter. In words, we choose the parameters under which the data we observed was the most likely outcome, assuming our model

We won't say much more about this process and this strategy other than to tie it back to some of the ideas laid out in Section \@ref(bayes-freq). By making an assumption about the model that best describes the process that generated the data, we are placing a distribution assumption on the data. The goal is to figure out which fixed parameters define a process that would have been most likely to produce an instance of the data like the one we observed.


### Bayesian

Before diving into the specifics of the Bayesian model fitting, recall that a Bayesian analysis proceeds by conditioning on the observed data and treating the unknown parameters as random. Importantly we still are interested in estimating the posterior $p(\theta \ | \ \text{data})$, but now we have many parameters of interest (i.e slope, intercept, and variance parameters) and so the expressions get a bit more complicated. As we get into all of the specifics that we lay out below, always remember that at the core of this process, we are treating our parameters as random and trying to quantify how they might vary given our observed data.

We'll start by describing the logistic regression model, before moving on to the two different versions of the generalized linear regression component.

#### Logistic Regression Component {#bayeslog}

Again we start by specifying the broad distribution of our response in this model

$$
z_{ij} \sim \text{Bernoulli}\Bigg(\frac{1}{1 + e^{-\mu_{ij}}}\Bigg) \qquad \text{where} \qquad \mu_{ij} =\mathbf{x}_{ij}^T\boldsymbol{\gamma} + v_j \ \ \ \text{and} \ \ \ v_j  \sim \mathcal{N}(0, \sigma_{v}^2)
$$

Here $\boldsymbol{\gamma} = (\gamma_1, ..., \gamma_P)$. Up until this point we have done nothing differently than in the usual Frequentist formulation of a model, but now instead of treating our model parameters as fixed but unknown, we treat them as random variables and attach priors to them. 

Since the fixed effects coefficients $\boldsymbol{\gamma}$ could plausibly be any real number, we'll utilize a normal distribution for their priors. On the other hand the variance parameter is necessarily strictly positive and so we use a Half-Cauchy prior for it.

$$
\begin{aligned}
\gamma_p &\sim \mathcal{N}(m_p, s_p^2)  \qquad \forall p\in 1, ...,P \\
\sigma_{v}^2 &\sim \text{Half-Cauchy}(0, r_1^2)
\end{aligned}
$$

The prior parameters $m_p, s_p^2, r_1^2$ are real-valued numbers that center and scale the priors, thus they are chosen with the specifics of the data set in mind. A Half-Cauchy distribution is just a Cauchy distribution bounded to non-negative values, and is broadly utilized as a prior for variance parameters due to the fact that "even in the tail, they have a gentle slope (unlike, for example, a half-normal distribution) and can let the data dominate if the likelihood is strong in that region."^[@gelman2006prior]. While dependence between priors *can* be modeled in a bayesian frame, we will assume that all of our priors are independent of each other. An example of what the prior distributions look like across a few of their parameter values is shown below.

```{r priors, echo = F, warning = F, message = F, fig.align = 'center', out.width="80%", fig.cap="Various parameter values for the prior distributions."}
library(tidyverse)
library(latex2exp)
library(patchwork)

normal <- ggplot(data.frame(x = c(-15, 10)), aes(x)) +
  stat_function(
    fun = dnorm, args = list(mean = 0, sd = 2),
    linewidth = 1.5, aes(color = "one")
  ) +
   stat_function(
    fun = dnorm, args = list(mean = 0, sd = 4),
    linewidth = 1.5, aes(color = "two")
  ) +
  stat_function(
    fun = dnorm, args = list(mean = 3, sd = 1),
    linewidth = 1.5, aes(color = "three")
  ) + 
  scale_color_manual(
    name  = "",
    breaks = c("one", "two", "three"),
    values = c("one" = "#b5e48c", "two" = "#34a0a4", "three" = "#184e77"),
    labels = unname(TeX(c("$m_p = 0, \ s_p = 2$", "$m_p = 1, \ s_p = 4$", "$m_p = 3, \ s_p = 1")))
  ) +
  theme_minimal() +
  labs(
    x = "X",
    y = "",
    title = unname(TeX("$Normal(m_p, s_p^2)$"))
  ) +
  theme(
    legend.position = c(.3, .8),
    legend.text = element_text(size = 8),
    plot.title = element_text(size = 11.5)
  ) 


cauchy <- ggplot(data.frame(x = c(0, 5)), aes(x)) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 2.5),
    linewidth = 1.5, aes(color = "one")
  ) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 1.5),
    linewidth = 1.5, aes(color = "two")
  ) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 0.5),
    linewidth = 1.5, aes(color = "three")
  ) +
  scale_color_manual(
    name  = "",
    breaks = c("one", "two", "three"),
    values = c("one" = "#faa307", "two" = "#d00000", "three" = "#370617"),
    labels = unname(TeX(c("$r_1 = 2.5$", "$r_1 = 1.5$", "$r_1 = 0.5")))
  ) +
  theme_minimal() +
  labs(
    x = "X",
    y = "",
    title = unname(TeX("Half-Cauchy$(0, r_1^2)$"))
  ) +
  theme(
    legend.position = c(.7, .8),
    legend.text = element_text(size = 8),
    plot.title = element_text(size = 11.5)
  ) 

normal + cauchy
```

Again, in estimating the actual **model parameters** in a Bayesian analysis the goal is to recover the joint posterior distribution of those parameters. As before, let $\boldsymbol{\gamma} = (\gamma_1, ...,\gamma_P)$ and $\mathbf{v} = (v_1, ..., v_J)$. The joint posterior distribution can be written as

$$
\begin{aligned}
p(\boldsymbol{\gamma}, \mathbf{v},  \sigma_v^2 \ | \ \mathbf{y}) &\propto \bigg[\prod_{i=1}^np(y_i \ | \ \boldsymbol{\gamma}, \mathbf{v},  \sigma_v^2)\bigg]\cdot p(\gamma_1, ...\gamma_P, v_1,... ,v_J, \sigma_{v}^2) \\
 &= \bigg[\prod_{i=1}^np(y_i \ | \ \boldsymbol{\gamma}, \mathbf{v},  \sigma_{v}^2)\bigg]\cdot p(\gamma_1)\cdot...\cdot p(\gamma_P)p(v_1) \cdot ... \cdot p(v_J)p(\sigma_{v}^2)
\end{aligned}
$$

If we were able to compute a closed form expression for the posterior, we could then attain posteriors for each of our individual model parameters by marginalizing- i.e integrating out all of the other parameters. For example, we might be interested in the posterior of only $\sigma_u^2$. In that case, the marginal posterior can be computed as follows:

$$
p(\sigma_{v}^2 \ | \ \mathbf{y}) = \int_{\gamma_1} \cdots \int_{\gamma_p} \int_{v_1} \dots \int_{v_J} p(\boldsymbol{\gamma}, \mathbf{v},  \sigma_{v}^2 \ | \ \mathbf{y}) d\beta_1 ... d\beta_P dv_1 ... dv_J
$$

The result would be a probability density function that encapsulates all of our information about $\sigma_{v}^2$.

As it turns out, the models are often complex enough that the RHS will not result in a recognizable probability density function. Thus we employ a Markov Chain Monte Carlo (MCMC) algorithm to simulate draws from the approximate posterior. To do so we use the probabilistic programming language "Stan". The specific version of MCMC algorithm that Stan runs is called "Hamiltonian Monte Carlo". While this thesis will not describe MCMC in depth, the short and sweet description is that an MCMC algorithm's strategy for drawing samples from an unknown probability distribution is to wander around the space in such a way that the amount of time spent in each location is proportional to the height of that distribution. The real nuts and bolts of the algorithm lie in how decisions are made about how and where to move around in the space so that the result is obtained. The especially powerful thing about MCMC algorithms is that under enough iterations, they construct a Markov Chain (random walk) that *has the desired posterior distribution as it's stationary distribution.* Thus we have to be a little bit careful with our language when interpreting the MCMC output. It's not that we are attaining samples from the actual posterior distribution (after all it is unknown), but rather stops along the random walk that is exploring the unknown posterior. That being said, if the MCMC algorithm converges properly then we will have samples from the approximate posterior distribution that should have characteristics similar to the actual posterior.

In it's most basic configuration Stan will output 2,000 sets of parameter draws which represent samples from the approximate joint posterior distribution.

$$
\begin{bmatrix}
  \gamma_1^{(1)} & \dots & \gamma_P^{(1)} & v_1^{(1)} & \dots & v_J^{(1)}  & (\sigma_{v}^2)^{(1)} \\
  \\ \vdots   &  & \vdots  & \vdots & &  \vdots & \vdots  \\ \\
  \gamma_1^{(2000)} & \dots &\gamma_P^{(2000)} & v_1^{(2000)}& \dots & v_J^{(2000)} & (\sigma_{v}^2)^{(2000)}
\end{bmatrix}
$$

One really nice aspect of this is that the while the combination of all of the columns in the output represent samples from the approximate **joint** posterior distribution, each column individually represent samples from the approximate **marginal** posterior distributions for that given individual parameter.

#### (Generalized) Linear Regression Component: Normal {#bayeslin}

The simplest way to model the non-zero response is through simple linear regression i.e generalized linear regression using the identity link and assuming a Normal distribution on the response. Again it may seem silly to introduce a simple linear regression model in this way, but we do so to stress that this form of the model still places just as many distributional assumptions as a more common GLM does.

$$
y^*_{ij} \ | \ \boldsymbol{\beta}, \boldsymbol{u}, \sigma_{u}^2,  \sigma_{\varepsilon}^2 \sim \mathcal{N}(\mu_{ij}, \sigma_{\varepsilon}^2) \qquad \text{where} \qquad \mu_{ij} = \mathbf{x}_{ij}^T\boldsymbol{\beta} + u_j \ \ \ \text{and} \ \ \ u_j \  \sim \mathcal{N}(0, \sigma_{u}^2)
$$

Here $\boldsymbol{\beta} = (\beta_1, ..., \beta_P)$. In this setting, the errors are assumed to be distributed $\mathcal{N}(0, \sigma_{\varepsilon}^2)$. And the other model parameters are given the same class of priors as before.

$$
\begin{aligned}
\beta_p &\sim \mathcal{N}(m_p, s_p^2)  \qquad \forall p\in 1, ..., P \\
\sigma_{\varepsilon}^2 &\sim \text{Half-Cauchy}(0, r_1^2) \\
\sigma_{u}^2 &\sim \text{Half-Cauchy}(0, r_2^2)
\end{aligned}
$$

Importantly while $s_p^2, r_1^2, r_2^2$ are given the same names as in the previous model, they should usually be chosen with the scale of the response in mind. 

We can write out our expression for the posterior as follows:

$$
\begin{aligned}
p(\boldsymbol{\beta}, \boldsymbol{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2 \ | \ \mathbf{y}) &\propto \bigg[\prod_{i:y_{i} > 0}p(y_{i} \ | \ \boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2)\bigg]\cdot p(\beta_1, ..., \beta_P, u_1, ..., u_J, \sigma_{u}^2, \sigma_{\varepsilon}^2) \\
 &=\bigg[\prod_{i:y_{i} > 0}p(y_{i} \ | \ \boldsymbol{\beta}, \mathbf{u},\sigma_{u}^2, \sigma_{\varepsilon}^2)\bigg]\cdot p(\beta_1)\cdot...\cdot p(\beta_P)p(u_1)\cdot ... \cdot p(u_J)p(\sigma_{u}^2)p( \sigma_{\varepsilon}^2) 
\end{aligned}
$$

Again, we employ MCMC using Stan to simulate draws from it. The Stan output will be 2000 draws from the approximate joint posterior distribution

$$
\begin{bmatrix}
  \beta_1^{(1)} & \dots & \beta_P^{(1)} & u_1^{(1)} & \dots & u_J^{(1)} & (\sigma_{u}^2)^{(1)} & (\sigma_{\varepsilon}^2)^{(1)} \\
  \\ \vdots &  & \vdots & \vdots & & \vdots & \vdots & \vdots \\ \\
  \beta_1^{(2000)} & \dots & \beta_P^{(2000)} & u_1^{(2000)} & \dots & u_J^{(2000)} & (\sigma_{u}^2)^{(2000)} & (\sigma_{\varepsilon}^2)^{(2000)}
\end{bmatrix}
$$

#### (Generalized) Linear Regression Component: Gamma

An alternative model that we considered in this thesis was a Gamma Generalized Linear Model. The motivation for this was to have a model that is more flexible to the distribution of the non-zero response. Figure \@ref(fig:gamma) displays several versions of a Gamma distribution with various parameters. In particular, note that the Gamma distribution is able to capture the fact that the non-zero response can be skewed in a way that a Normal distribution cannot.

```{r gamma, echo = F, warning = F, message = F, fig.align = 'center', out.width="80%", fig.cap="The Gamma(a, b) Distribution"}
gamma <- ggplot(data.frame(x = c(0, 20)), aes(x)) +
  stat_function(
    fun = dgamma, args = list(shape = 2, rate = 0.5),
    linewidth = 1.5, aes(color = "one")
  ) +
   stat_function(
    fun = dgamma, args = list(shape = 9.5, rate = 2),
    linewidth = 1.5, aes(color = "two")
  ) +
   stat_function(
    fun = dgamma, args = list(shape = 7.5, rate = 1),
    linewidth = 1.5, aes(color = "three")
  ) +
  scale_color_manual(
    name  = "",
    breaks = c("one", "two", "three"),
    values = c("one" = "#2b66c4", "two" = "#b1cefc", "three" = "#041736"),
    labels = unname(TeX(c("$a = 2, b = 0.5 $", "$a = 9.5, b = 2$", "$a = 7.5, b = 1")))
  ) +
  theme_minimal() +
  labs(
    x = "X",
    y = ""
  ) +
  theme(
    legend.position = c(.7, .8),
    legend.text = element_text(size = 8),
    plot.title = element_text(size = 11.5)
  )

gamma
```


Because the response variable we are working with is strictly positive and often-times right skewed, it's logical to try to model the response as coming from a Gamma distribution. The model is formulated as follows

$$
y_{ij} \sim \text{Gamma}\bigg(\alpha, \frac{\alpha}{\mu_{ij}}\bigg) \qquad \text{where} \qquad \mu_{ij} = \mathbf{x}_{ij}^T\boldsymbol{\beta} + u_j \ \ \ \text{and} \ \ \ u_j  \sim \mathcal{N}(0, \sigma_{u}^2)
$$

We are operating under the shape and rate parametrization of a Gamma distribution, and this specific parametrization was chosen so the mean of our response would be the output of the linear component:

$$
E[y_{ij}] = \alpha \bigg(\frac{\alpha}{\mu_{ij}}\bigg)^{-1} = \mu_{ij}
$$

We then define priors for the other parameters as

$$
\begin{aligned}
\beta_p &\sim \mathcal{N}(m_p, s_p^2)  \qquad \forall p\in P \\
\alpha &\sim \text{Half-Cauchy}(t_1, r_1^2)\\
\sigma_{u}^2 &\sim \text{Half-Cauchy}(0, r_2^2)
\end{aligned}
$$

where $m_p, s_p^2, t_1, r_1, r_2$ are real valued numbers. While the Cauchy prior put on the shape parameter $\alpha$ is still bounded below by zero, we now allow it's mode to be set based on the given data.

Letting $\boldsymbol{\beta} = (\beta_1, ..., \beta_P)$ and $\mathbf{u} = (u_1, ..., u_J)$ we can write out the joint posterior distribution as

$$
\begin{aligned}
p(\boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \alpha \ | \ \mathbf{y}) &=\bigg[\prod_{i:y_{i} > 0}p(y_{i} \ | \ \boldsymbol{\beta},\mathbf{u}, \sigma_{u}^2, \alpha)\bigg]\cdot p(\beta_1)\cdot...\cdot p(\beta_P)p(u_1)\cdot ... \cdot p(u_J)p(\sigma_{u}^2)p( \alpha) 
\end{aligned}
$$

Hopefully at this point the repetitiveness of this process and of these formulations has helped to drill home the Bayesian method of model fitting. We've seen it in three different flavors: Logistic regression, Normal regression, and Gamma regression, but all that's really changed at each step has been the link function and the priors and parameters used.

## Evaluation of the Bayesian Model: Posterior Predictive Distribution {#bayespred}

In the Frequentist frame the model parameters are treated as fixed but unknown and so once we obtain estimates for them, we simply use those point estimates to make predictions.

$$
\begin{aligned}
\hat{y}^*_{ij} &= \mathbf{x}_{ij}^T\hat{\boldsymbol{\beta}} + \hat{u}_j \\
\hat{p}_{ij} &= \frac{1}{1 + e^{-(\mathbf{x}_{ij}^T\hat{\boldsymbol{\gamma}} + \hat{v}_j)}} \\
\hat{Y}_{j} &= \frac{1}{U_j}\sum_{i\in U_j}\hat{y}^*_{ij}\cdot \hat{p}_{ij} 
\end{aligned}
$$

But in the Bayesian frame our model parameters are no longer fixed values, but are described by a posterior distribution. Instead of producing predictions that are single values, we construct what are called posterior predictive distributions. In fact it makes sense why we would end up with predictive distributions rather than single values when you consider that there are two main sources of variability that should be taken into account in our predictions:

1.  Sampling variability in the data: we never expect our model to be perfectly deterministic, rather the real outcomes should be expected to vary around the model.

2.  Posterior variability of the model parameters: we shouldn't go through all the trouble of constructing posterior distributions for our parameters to just throw out that information when it comes time to make predictions, rather we incorporate the variability in our posterior distributions into our predictions.

Within the Bayesian frame, these two sources of variability are combined to produce what is called a posterior predictive distribution [@johnson2022bayes].

To get a feel for how this works, I'll start by just focusing on constructing a posterior predictive distribution for a new point, $y_{ij, \ new}$, using each model separately.

### Theoretical version {#predtheory}

To really stress the logic of what we're doing, imagine that we haven't collected any data yet and that we only had one parameter $A$ in our model. In any model we assume that the data for a fixed parameter $A$ has distribution $p(y \ | \ A)$. Moreover, before having observed any data, all of our uncertainty about the value of $A$ is contained by the prior $p(A)$. We can imagine $p(y \ | \ A)$ capturing the variability in (1) and $p(A)$ capturing the variability in (2) as listed above.

To produce an estimate for the distribution of a new data point $y_{ij, \ new}$ we simply integrate the product of the previous two terms over $A$.


$$
\begin{aligned}
\int_A p(y_{ij, \ new} \ | \ A)p(A )dA   = p(y_{ij, \ new} )   \\
\end{aligned}
$$

This is sometimes called the prior predictive distribution for $y_{ij, \ new}$ as it represents our knowledge about $y_{ij, \ new}$ before observing any data. 

But we can do much better at describing the variability of the model parameters than this. After observing the sample data $\mathbf{y}$ we update our knowledge. Again we have the same $p(y_{ij, \ new} \ | \ A)$ which captures the sampling variability in the data, but now the variability of the model parameter is described by our posterior distribution $p(A \ | \ \mathbf{y})$. Again the goal is to get a distribution for $y_{ij, \ new}$ and this time which we can express as follows. This new distribution that takes into account the observed data is called the posterior predictive distribution.


$$
\begin{aligned}
p(y_{ij,\ new} \ | \ \mathbf{y}) &= 
\int_A p(y_{ij, \ new} \ | \ A, \mathbf{y})p(A \ | \ \mathbf{y})dA \\ &=  \int_A p(y_{ij, \ new} \ | \ A)p(A \ | \ \mathbf{y})dA \qquad y_{ij, \ new} \ \  \text{independent of } \mathbf{y} 
\end{aligned}
$$

Note that the quality of this posterior predictive distribution depends strongly on the quality of our posterior distribution. In other words, this distribution will only accurately capture the structure of new data points, if the underlying posterior distribution correctly captures the structure of the parameters of interest.

As we move on to the expression for each full model, just remember that while the integral looks very complicated, all that we're doing is incorporating both sources of variability and averaging across the possible values of the model parameters. The posterior predictive distribution for a point $y^*_{ij, \ new}$ using the Normal regression model can be written as:

$$
\begin{aligned}
p(y^*_{ij, \ new} \ | \ \mathbf{y}) &=  \int_{\beta_1}  \dots \int_{\beta_P} \int_{u_1} \dots \int_{u_J} \int_{\sigma_{u}^2} \int_{\sigma_{\varepsilon}^2} \bigg[p(y^*_{ij, \ new} \ | \ \boldsymbol{\beta}, \boldsymbol{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2)p(\boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2 \ | \ \mathbf{y})\bigg] \\
& \ \ \ \ \ d\beta_1 ...  d\beta_Pdu_1...du_Jd\sigma_{u}^2d\sigma_{\varepsilon}^2
\end{aligned}
$$

Similarly, for the Gamma model it can be written as

$$
\begin{aligned}
p(y^*_{ij, \ new} \ | \ \mathbf{y}) &=  \int_{\beta_1}  \dots \int_{\beta_P} \int_{u_1} \dots \int_{u_J} \int_{\sigma_{u}^2} \int_{\alpha} \bigg[p(y^*_{ij, \ new} \ | \ \boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \alpha)p(\boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \alpha \ | \ \mathbf{y})\bigg] \\
& \ \ \ \ \ d\beta_1 ...  d\beta_Pdu_1...du_Jd\sigma_{u}^2d\alpha
\end{aligned}
$$

And for the classification model it can be expressed as

$$
\begin{aligned}
p(z_{ij, \ new} \ | \ \mathbf{y}) &=  \int_{\gamma_1}  \dots \int_{\gamma_P} \int_{v_1} \dots \int_{v_J} \int_{\sigma_{v}^2}  \bigg[p(z_{ij, \ new} \ | \ \boldsymbol{\gamma}, \mathbf{v}, \sigma_{v}^2)p(\boldsymbol{\gamma}, \mathbf{v}, \sigma_{v}^2 \ | \ \mathbf{y})\bigg] \\
& \ \ \ \ \ d\gamma_1 ...  d\gamma_Pdv_1...dv_Jd\sigma_{v}^2
\end{aligned}
$$

The distributions $p(y^*_{ij, \ new} \ | \ \mathbf{y})$ and $p(z_{ij, \ new}\ | \ \mathbf{y})$ not only capture where we think each prediction might lie, but also how we would expect it to vary. Often in predictive modeling we're interested in quantifying the uncertainty in our model estimates, and in the Bayesian framework these are baked right into the predictions themselves.

While the theory behind constructing these posterior predictive is pretty intuitive, it's clear that even in the case of a fairly simple model, the actual computations are rather unwieldy. Again, we are saved by the fact that in practice the posterior is too complex to algebraically solve for, so we're already functioning in a setting where we use MCMC to simulate draws from the approximate posterior.

### MCMC version

First I will describe how the posterior predictive distribution is derived from the MCMC draws, and then I will explain how it approximates the exact calculation above.

To generate a posterior predictive distribution for a new data point $y_{ij,\ new}$ using the normal regression model we simulate a prediction from the model for each parameter set of the MCMC output. Recall that the superscripts $(i)$ index the MCMC iteration.

$$
\begin{bmatrix}
y_{ij, \ new}^{*^{(1)}} \sim \mathcal{N}\Big( \mathbf{x}_{ij, \ new}^T \boldsymbol{\beta}^{(1)} + u_j^{(1)}, (\sigma_{\varepsilon}^2)^{(1)}\Big) \\
\\
\vdots \\
\\
y_{ij, \ new}^{*^{(2000)}} \sim \mathcal{N}\Big(\mathbf{x}_{ij, \ new}^T \boldsymbol{\beta}^{(2000)} + u_j^{(2000)}, (\sigma_{\varepsilon}^2)^{(2000)}\Big)
\end{bmatrix}
$$

The result is a set $\Big\{y_{ij , \ new}^{*^{(1)}}, y_{ij, \ new}^{*^{(2)}}, ..., y_{ij, \ new}^{*^{(2000)}}\Big\}$ which approximates the posterior predictive distribution.

For the Gamma model we do the same thing but using the appropriate distribution

$$
\begin{bmatrix}
y_{ij, \ new}^{*^{(1)}} \sim \text{Gamma}\Bigg(\alpha^{(1)}, \frac{\alpha^{(1)}}{ \mathbf{x}_{ij, \ new}^T\cdot \boldsymbol{\beta}^{(1)} + u_j^{(1)}}\Bigg) \\
\\
\vdots \\
\\
y_{ij, \ new}^{*^{(2000)}} \sim \text{Gamma}\Bigg(\alpha^{(2000)}, \frac{\alpha^{(2000)}}{ \mathbf{x}_{ij, \ new}^T\cdot \boldsymbol{\beta}^{(2000)} + u_j^{(2000)}}\Bigg)
\end{bmatrix}
$$

And we do the same thing for the classification model

$$
\begin{bmatrix}
z_{ij, \ new}^{(1)} \sim \text{Bernoulli}\Bigg(\frac{1}{1 + e^{-\big(\mathbf{x}_{ij}^T\boldsymbol{\gamma}^{(1)} + v_j^{(1)}\big)}}\Bigg) \\
\\ \vdots \\ \\
z_{ij, \ new}^{(2000)} \sim \text{Bernoulli}\Bigg(\frac{1}{1 + e^{-\big(\mathbf{x}_{ij}^T\boldsymbol{\gamma}^{(2000)} + v_j^{(2000)}\big)}}\Bigg)
\end{bmatrix}
$$

While it may not be immediately clear, these processes are really just mimicking what the massive integrals above were computing exactly. By simulating realizations of the distribution behind each model, we are again capturing the sampling variability in the data, and by doing so across all of our MCMC parameter draws, the uncertainty about the model parameters ($p(A \ | \ \mathbf{y} )$ from the \@ref(predtheory)) is being incorporated as well.

### Combining the Model Predictions {#comb}

Now that we have two sets which represent approximate the posterior predictive distribution for unit $i$ in group $j$ for each respective model, we have to think about how we combine them. After all, our final model prediction is the product of these two models, so we certainly need a posterior predictive distribution of $y_{ij, \ new} = y_{ij, \ new}^*z_{ij, \ new}$, but it's unclear how we should combine the predictive distributions from the individual models to get here. In the Frequentist version where our predictions are single point values, this poses no problem at all, but now that our predictions are themselves distributions, it's a little less clear how to proceed. One idea is to just match MCMC iteration $k$ from each model together. For example, when using the GLM with normal distribution and identity link as well as the logistic regression model we might just proceed as follows:

$$
\Big\{y_{ij , \ new}^{*^{(1)}}\cdot z^{(1)}_{ij, \ new}, \  ... \ , y_{ij, \ new}^{*^{(2000)}}\cdot z^{(2000)}_{ij, \ new}\Big\}
$$

But what makes this matching more correct than shuffling the iterations and then matching them up?

One solution to this conundrum of combining the distributions is to simply build the models simultaneously. In practice this relies on a few tricks and definitely increases the complexity when actually writing code for it, but it can be done and it does allow us to avoid this problem. That being said, as the two models grow to be more complicated, this process of building them simultaneously grows much more difficult and so it isn't a very robust solution to the problem. But, it turns out that with a few minimal assumptions, we can show that the full posterior for the model built simultaneously is equal to the product of the posteriors for each model built separately. What this means in practice is that we can actually fit the models separately (as we have been doing) and then combine them at the end, and the result will be exactly the same as if we fit the models simultaneously.

We thought that the proof and description of this finding deserved it's own chapter, so it is located in Chapter \@ref(sepsim). That being said, we'll use the result to finish up this section on prediction.


### Posterior Predictive Distribution Finalized {#wrapone}

Now that we know that we can fit the models separately and then combine them at the end, we are finally ready to describe how final predictions are made.

Since we are interested in a making predictions for the average response in group $j$ we obtain a set that approximates the posterior predictive distribution for each unit $i$ in group $j$. For example, if we fix $j$ and let $U_j = 5$ we would have 5 sets:

$$
\begin{aligned}
&\Big\{y_{1j , \ new}^{(1)}, \ y_{1j, \ new}^{(2)}, ..., \ y_{1j, \ new}^{(2000)}\Big\} \\
&\Big\{y_{2j , \ new}^{(1)}, \ y_{2j, \ new}^{(2)}, ...,  \ y_{2j, \ new}^{(2000)}\Big\} \\
& \qquad \qquad \qquad \ \   \vdots \\
&\Big\{y_{5j , \ new}^{(1)}, \ y_{5j, \ new}^{(2)}, ..., \ y_{5j, \ new}^{(2000)}\Big\}
\end{aligned}
$$

To get the posterior predictive distribution for the mean in group j $Y_{j, \ new}$ we take averages across units in group $j$ by indices of the MCMC draws. In other words we take the average of $\Big\{y_{1j, \ new}^{(k)}, y_{2j, \ new}^{(k)}, ... , y_{5j, \ new}^{(k)}\Big\}$ for each MCMC draw $k$. In full we end up with the set:

$$
\begin{bmatrix}
\frac{1}{U_j} \sum_{i \in U_j}\hat{y}_{ij, \ new}^{(1)} \\
\frac{1}{U_j} \sum_{i \in U_j}\hat{y}_{ij, \ new}^{(2)} \\
\vdots \\
\frac{1}{U_j} \sum_{i \in U_j}\hat{y}_{ij, \ new}^{(2000)}
\end{bmatrix} \qquad = \qquad 
\begin{bmatrix}
\hat{Y}_{j, \ new}^{(1)} \\
\hat{Y}_{j, \ new}^{(2)} \\
\vdots \\
\hat{Y}_{j, \ new}^{(2000)}
\end{bmatrix}
$$

Which is an approximation of the posterior predictive distribution for the mean of the response in group $j$.

### Prediction Intervals {#wraptwo}

In statistical modeling, another piece of information that we're often interested is a measure of our uncertainty in our predicted values. Often times these are referred to broadly as prediction intervals, and whereas the confidence intervals and credible intervals that we described in Chapter \@ref(bayes-freq) provide uncertainty bounds for parameter estimates, these prediction intervals provide uncertainty bounds for a future observation or data point.

Immediately, we can see how straightforward it is to acquire these in the Bayesian setting. Because our predictions are not simply point estimates, but are distributions themselves, these prediction intervals are baked right into the prediction process. 

Unfortunately it is far less straightforward in the Frequentist setting. While prediction intervals are straightforward to generate for singular regression models, things get a lot more complicated with a two part model like ours. The reality is that we spent a good amount of time trying to construct a bootstrap procedure to generate these prediction intervals and beyond being very computationally intensive, we also couldn't find one that actually worked correctly. While this was certainly frustrating, the struggle to try to develop a process for generating these prediction intervals for the Frequentist models really highlighted how nice it is that you get them for free in the Bayesian model.

#### A note of caution

While it might be tempting to gleefully pronounce Bayesian models to be better than Frequentist simply for the ease of access to prediction intervals, the important caveat is that these prediction intervals are only assumed to be correct when the Bayesian model is correct. What I mean by that is that if our Bayesian model badly fits the data, then we still get uncertainty estimates for free, but a 95% prediction interval will likely not get 95% coverage, thus indicating that the intervals themselves are also incorrect. 

