# Methods {#methods-sec}

We now move into a thorough description of all of the methods employed in this thesis. In subsections 3.1-3.3 the necessary notation and model structure are introduced. While the rest of the subsections intermittently mention the Frequentist model, they are primarily focused on the Bayesian models. In particilar, 3.4 describes how the Bayesian models are fit, 3.5 describes what prediction with a Bayesian model looks like, 3.6 presents a theoretical backing for why the components of the Bayesian two-part model can be fit separately, and finally 3.7 wraps everything up.


## Notation

Let $U$ denote a finite population with $N$ elements. $U$ is broken into $J$ groups $U_j$, $j = 1, 2, ..., J$, where each group $U_j$ is defined as having $n_j$ sample observations. Let $p = 1, ..., P$ index the covariates. Each sample observation, $i$ in group $j$ has auxiliary information $x_{ij}^p$ for covariate $p$, response value $y_{ij}$, and indicator for being non-zero $z_{ij}$.

$$
z_{ij} =
\begin{cases}
1 & \text{if}\ \ y_{ij} \ne 0 \\
0 & \text{if} \ \ y_{ij} = 0
\end{cases}
$$


## Model Structure Formalized

We now introduce the modeling technique that will be the main focus of the thesis. Wonderfully, there is some real mathematical backing for why we might build this model in the way that we do, and I think that looking at the steps draws out a lot of helpful intuition.

Let $Y$ represent the response variable and $X$ represent the covariates. We typically write $\mathbb{E}[Y \ | \ X]$ to denote the expected value of our response variable conditional on it's covariates. Since the separation of the response into values that are zero and those that are not is a finite partition, we can leverage the law of iterated expectation to expand our model structure:

\begin{equation}
\begin{split}
    E[ Y  \ | \ X = x] &= \underbrace{E[Y \ | \ X = x, Y = 0]}_{= \ 0}P(Y = 0 \ | \ X = x) + E[Y \ | \ X = x, \ Y > 0]P(Y> 0 \ | \ X = x) \\
    &= E[Y \ | \ X = x, \ Y > 0]P(Y > 0 \ | \ X = x) 
\end{split}
(\#eq:intuition)
\end{equation}


Out of this equation comes a wonderful intuition for what our new modeling process will look like, what we end up with is something that is somewhat meta in that it doesn't tell us what the exact model will be, but rather it tells us what the structure of our model should look like. What we do know is that we should have one model that predicts our non-zero response using our covariates $E[Y \ | \ X = x, \ Y>0]$, and another that predicts whether our response is non-zero or not, again using the covariates $P(Y > 0 \ | \ X = x)$.

Think back to the Introduction where we introduced the model as being:

$$
\text{Final Prediction} = \bigg(\text{Regression Model Output}\bigg) \times \bigg(\text{Classification Model Output}\bigg)
$$

all we've done above is provided a formal theoretical backing for this structure and strategy.

Importantly, while Equation \@ref(eq:intuition) describes the broader two-part model strategy that will be employed by both the Frequentist and Bayesian versions of the model, the formulation can't technically be extended into the context of prediction for the Bayesian model. Equation \@ref(eq:intuition) does describe how prediction functions in the Frequentist case, but as we will get to later in the Bayesian setting everything is described by a probability distribution and so we aren't working with expected values anymore. That being said, this is just a small technicality that shouldn't take away from the really nice larger intuition that Equation \@ref(eq:intuition) holds for both models.

## Specific Models

Although we could model these two parts however we wanted to, we will use a generalized linear regression model (that is non-logistic) for the first part and a logistic regression model for the second. Moreover, because the data we will be working with has a clustered structure we attempt to capture that by including group-level random effects in both models. The precise models that we will be evaluating will be as follows.

We fit a generalized linear model with random intercepts to the **non-zero** portion of the data (the $*$ helps differentiate this model from our final model). The linear predictor is specified as follows

$$
\mu_{ij} = \mathbf{x}_{ij}^T\boldsymbol{\beta} + u_j + \varepsilon_{ij} \qquad \text{where} \qquad u_j \sim \mathcal{N}(0, \sigma_{u}^2)
$$

With a link function $g^{-1}$ and a probability distribution for the response, we get the final model as

$$
y^*_{ij} = g^{-1}(\mu_{ij})
$$

At this point we don't place a specific distributional assumption on the error term $\varepsilon_{ij}$ because this will vary with the specific generalized linear model being employed. To spoil the surprise, the two GLMs that we will look at will be one with identity link and Normal distribution (i.e normal linear regression), and one with the log link and a Gamma distribution. That being said, we keep the notation broad at this point so as to emphasize the fact that this portion of the model aims to capture the structure of the non-zero response, and even though we try to model it using various distributions, this is still the main goal.

Here, $\mathbf{x}_{ij}^T = (x^1_{ij}, ..., x^P_{ij})$ is a $P\times 1$ vector of covariates, $\boldsymbol{\beta}$ is a $1\times P$ vector of fixed effects, and $u_j$ is the random effect associated with group $j$. Finally, $\sigma^2_{u}$ is the between group variance parameter. The distribution on $\varepsilon_{ij}$ will change depending on the specifics of the glm that we employ.

Next we fit a logistic regression random intercepts model to the full data set

$$
P(z_{ij} = 1) = p_{ij} = \frac{1}{1 + e^{-(\mathbf{x}_{ij}^T\boldsymbol{\gamma} + v_j)}} \qquad \text{where} \qquad v_j \sim \mathcal{N}(0, \sigma_{v}^2)
$$

Here $\boldsymbol{\gamma}$ is a $1\times P$ vector of fixed effects and $v_j$ is the random effect associated with group $j$. Again, $\sigma^2_{v}$ is the between group variance parameter.

We will get into how each of these model pieces are estimated in the next section, but once we have estimates for all of our coefficients, we get our final model by taking the product of these two estimated models.

$$
\hat{y}_{ij} = \hat{y}^*_{ij}\cdot \hat{p}_{ij}
$$

And we get a prediction for a single group by averaging the predictions over all samples in that group.

$$
\hat{Y}_j = \frac{1}{n_j}\sum_{i \in n_j}\hat{y}^*_{ij}\cdot \hat{p}_{ij}
$$

Importantly, while $y^*_{ij}$ is fit on only the non-zero data, and $p_{ij}$ is fit on the entire data, when we make predictions on new data, both models are applied to the entire new data set.

## Model Fitting: Two Ways

### Frequentist

As it is not the focus of this thesis, we will not go in depth into how these models are fit in a Frequentist setting. Still, it's important to at least provide a brief summary of how it is most often done. 

In most cases (and in particular in most statistical software), Frequentist regression models are fit using something called Maximum Likelihood Estimation (MLE). Very broadly, MLE functions by first assuming that the observed data was sampled from some distribution. Out of that assumption we get a likelihood function $p(\text{data} \ | \ \text{parameters})$. And finally, as the name suggests, we choose parameter values that maximize the likelihood of the observed data given that parameter. The main gist of what's happening here is that we are answering the question: under what fixed parameter values would we be most likely to see the data that we observed? In the case of a statistical model this translates into: what parameter values maximize the likelihood that the process described by the model produced the data that were actually observed?

We won't say much more about this process and this strategy other than to tie it back to some of the ideas laid out in Section \@ref(bayes-freq). By making an assumption about the model that best describes the process that generated the data, we are conceptualizing the data as being random. The goal is to figure out which fixed parameters define a process that would have been most likely to produce an instance of the data like the one we observed.


### Bayesian

Before diving into the specifics of the Bayesian model fitting, recall that a Bayesian analysis proceeds by treating the data as fixed and the unknown parameters as random. Importantly we still are interested in estimating the posterior $p(\theta \ | \ \text{data})$, but now we have many parameters of interest $p(\beta_1, \beta_2, \sigma^2_{u}, ... \ \text{etc} \ | \ \text{data})$, and so the expressions get a bit more complicated. As we get into all of the specifics that we lay out below, always remember that at the core of this process, we aree= treating our parameters as random and trying to quantify how they might vary given our fixed data.

We'll start by describing the logistic regression model, before moving on to the two different versions of the generalized linear regression component.

#### Logistic Regression Component

Again we start by specifying the broad distribution of our response in this model

$$
z_{ij} \sim \text{Bernoulli}\Bigg(\frac{1}{1 + e^{-\mu_{ij}}}\Bigg) \qquad \text{where} \qquad \mu_{ij} =\mathbf{x}_{ij}^T\boldsymbol{\gamma} + v_j
$$

So far we have done nothing differently than in the usual frequentist formulation of a model, but now instead of treating our model parameters as fixed but unknown, we treat them as random variables and attach priors to them. The random intercepts are given a normal prior centered on zero with hyper-prior $\sigma_v^2$.

$$
v_j \ | \ \sigma_{v}^2 \sim \mathcal{N}(0, \sigma_{v}^2)
$$

Although $\boldsymbol{\gamma}$ and $\sigma_{v}^2$ are different parameters than the one's in the previous model, we attach the same priors to them

$$
\begin{aligned}
\gamma_p &\sim \mathcal{N}(m_p, s_p^2)  \qquad \forall p\in 1...P \\
\sigma_{v}^2 &\sim \text{Half-Cauchy}(0, r_1^2)
\end{aligned}
$$

The prior parameters $m_p, s_p^2, r_1^2$ are real-valued numbers that center and scale the priors, thus they are chosen with the specifics of the data set in mind. A Half-Cauchy distribution is just a Cauchy distribution bounded to non-negative values, and is broadly utilized as a prior for variance parameters due to the fact that "even in the tail, they have a gentle slope (unlike, for example, a half-normal distribution) and can let the data dominate if the likelihood is strong in that region."^[@gelman2006prior]. While dependence between priors *can* be modeled in a bayesian frame, we will assume that all of our priors are independent of each other.

An example of what the prior distributions look like across a few of their parameter values is shown below.

```{r priors, echo = F, warning = F, message = F, fig.align = 'center', out.width="80%", fig.cap="Various parameter values for the prior distributions."}
library(tidyverse)
library(latex2exp)
library(patchwork)

normal <- ggplot(data.frame(x = c(-15, 10)), aes(x)) +
  stat_function(
    fun = dnorm, args = list(mean = 0, sd = 2),
    linewidth = 1.5, aes(color = "one")
  ) +
   stat_function(
    fun = dnorm, args = list(mean = 0, sd = 4),
    linewidth = 1.5, aes(color = "two")
  ) +
  stat_function(
    fun = dnorm, args = list(mean = 3, sd = 1),
    linewidth = 1.5, aes(color = "three")
  ) + 
  scale_color_manual(
    name  = "",
    breaks = c("one", "two", "three"),
    values = c("one" = "#b5e48c", "two" = "#34a0a4", "three" = "#184e77"),
    labels = unname(TeX(c("$m_p = 0, \ s_p = 2$", "$m_p = 1, \ s_p = 4$", "$m_p = 3, \ s_p = 1")))
  ) +
  theme_minimal() +
  labs(
    x = "X",
    y = "",
    title = unname(TeX("$Normal(m_p, s_p^2)$"))
  ) +
  theme(
    legend.position = c(.3, .8),
    legend.text = element_text(size = 8),
    plot.title = element_text(size = 11.5)
  ) 


cauchy <- ggplot(data.frame(x = c(0, 5)), aes(x)) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 2.5),
    linewidth = 1.5, aes(color = "one")
  ) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 1.5),
    linewidth = 1.5, aes(color = "two")
  ) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 0.5),
    linewidth = 1.5, aes(color = "three")
  ) +
  scale_color_manual(
    name  = "",
    breaks = c("one", "two", "three"),
    values = c("one" = "#faa307", "two" = "#d00000", "three" = "#370617"),
    labels = unname(TeX(c("$r_1 = 2.5$", "$r_1 = 1.5$", "$r_1 = 0.5")))
  ) +
  theme_minimal() +
  labs(
    x = "X",
    y = "",
    title = unname(TeX("Half-Cauchy$(0, r_1^2)$"))
  ) +
  theme(
    legend.position = c(.7, .8),
    legend.text = element_text(size = 8),
    plot.title = element_text(size = 11.5)
  ) 

normal + cauchy
```

Again, in estimating the actual **model parameters** in a Bayesian analysis the goal is to recover the joint posterior distribution of those parameters. Let $\boldsymbol{\gamma} = (\gamma_1, ...,\gamma_P)$ and $\mathbf{v} = (v_1, ..., v_J)$. The joint posterior distribution can be written as

$$
\begin{aligned}
p(\boldsymbol{\gamma}, \mathbf{v},  \sigma_v^2 \ | \ \mathbf{y}) &\propto \bigg[\prod_{i=1}^np(y_i \ | \ \boldsymbol{\gamma}, \mathbf{v},  \sigma_v^2)\bigg]\cdot p(\gamma_1, ...\gamma_P, v_1,... ,v_J, \sigma_{v}^2) \\
 &= \bigg[\prod_{i=1}^np(y_i \ | \ \boldsymbol{\gamma}, \mathbf{v},  \sigma_{v}^2)\bigg]\cdot p(\gamma_1)\cdot...\cdot p(\gamma_P)p(v_1) \cdot ... \cdot p(v_J)p(\sigma_{v}^2)
\end{aligned}
$$

If we were able to compute a closed form expression for the posterior, we could then attain posteriors for each of our individual model parameters by marginalizing- i.e integrating out all of the other parameters. For example, we might be interested in the posterior of only $\sigma_u^2$. In that case, the marginal posterior can be computed as follows:

$$
p(\sigma_{v}^2 \ | \ \mathbf{y}) = \int_{\gamma_1} \cdots \int_{\gamma_p} \int_{v_1} \dots \int_{v_J} p(\boldsymbol{\gamma}, \mathbf{v},  \sigma_{v}^2 \ | \ \mathbf{y}) d\beta_1 ... d\beta_P dv_1 ... dv_J
$$

The result would be a probability density function that encapsulates all of our information about $\sigma_{v}^2$.

As it turns out, the models are often complex enough that the RHS will not result in a recognizable probability density function. Thus we employ a Markov Chain Monte Carlo (MCMC) algorithm to simulate draws from the approximate posterior. To do so we use the probabilistic programming language "Stan". The specific version of MCMC algorithm that Stan runs is called "Hamiltonian Monte Carlo". While this thesis will not describe MCMC in depth, the short and sweet description is that an MCMC algorithm's strategy for drawing samples from an unknown probability distribution is to wander around the space in such a way that the amount of time spent in each location is proportional to the height of that distribution. The real nuts and bolts of the algorithm lie in how decisions are made about how and where to move around in the space so that the result is obtained. The especially powerful thing about MCMC algorithms is that under enough iterations, they construct a Markov Chain (random walk) that *has the desired posterior distribution as it's stationary distribution.* Thus we have to be a little bit careful with our language when interpreting the MCMC output. It's not that we are attaining samples from the actual posterior distribution (after all it is unknown), but rather stops along the random walk that is exploring the unknown posterior. That being said, if the MCMC algorithm converges properly then we will have samples from the approximate posterior distribution that should have characteristics similar to the actual posterior.

In it's most basic configuration Stan will output 2,000 sets of parameter draws which represent samples from the approximate joint posterior distribution.

$$
\begin{bmatrix}
  \gamma_1^{(1)} & \dots & \gamma_P^{(1)} & v_1^{(1)} & \dots & v_J^{(1)}  & (\sigma_{v}^2)^{(1)} \\
  \\ \vdots   &  & \vdots  & \vdots & &  \vdots & \vdots  \\ \\
  \gamma_1^{(2000)} & \dots &\gamma_P^{(2000)} & v_1^{(2000)}& \dots & v_J^{(2000)} & (\sigma_{v}^2)^{(2000)}
\end{bmatrix}
$$

One really nice aspect of this is that the while the combination of all of the columns in the output represent samples from the approximate **joint** posterior distribution, each column individually represent samples from the approximate **marginal** posterior distributions for that given individual parameter.

#### (Generalized) Linear Regression Component: Normal

The simplest way to model the non-zero response is through simple linear regression i.e generalized linear regression using the identity link and assuming a Normal distribution on the response. Again it may seem silly to introduce a simple linear regression model in this way, but we do so to stress that this form of the model still places just as many distributional assumptions as a more common GLM does.

$$
y^*_{ij} \ | \ \boldsymbol{\beta}, \boldsymbol{u}, \sigma_{u}^2,  \sigma_{\varepsilon}^2 \sim \mathcal{N}(\mu_{ij}, \sigma_{\varepsilon}^2) \qquad \text{where} \qquad \mu_{ij} = \mathbf{x}_{ij}^T\boldsymbol{\beta} + u_j + \varepsilon_{ij}
$$

In this setting, the error term is assumed to be distributed $\mathcal{N}(0, \sigma_{\varepsilon}^2)$.

Again the random intercepts have priors

$$
u_j \ | \ \sigma_{u}^2 \sim \mathcal{N}(0, \sigma_{u}^2)
$$

with hyperprior (i.e priors put on a hyperparameter) $\sigma_{u}^2$.

And the other model parameters are given the same class of priors as before.

$$
\begin{aligned}
\beta_p &\sim \mathcal{N}(m_p, s_p^2)  \qquad \forall p\in P \\
\sigma_{\varepsilon}^2 &\sim \text{Half-Cauchy}(0, r_1^2) \\
\sigma_{u}^2 &\sim \text{Half-Cauchy}(0, r_2^2)
\end{aligned}
$$

Importantly while $s_p^2, r_1^2, r_2^2$ are given the same names as in the previous model, they should usually be chosen with the scale of the response in mind. Let $\boldsymbol{\beta} = (\beta_1, ..., \beta_P)$ and let $\mathbf{u} = (u_1, ..., u_J)$.

$$
\begin{aligned}
p(\boldsymbol{\beta}, \boldsymbol{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2 \ | \ \mathbf{y}) &\propto \bigg[\prod_{i:y_{i} > 0}p(y_{i} \ | \ \boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2)\bigg]\cdot p(\beta_1, ..., \beta_P, u_1, ..., u_J, \sigma_{u}^2, \sigma_{\varepsilon}^2) \\
 &=\bigg[\prod_{i:y_{i} > 0}p(y_{i} \ | \ \boldsymbol{\beta}, \mathbf{u},\sigma_{u}^2, \sigma_{\varepsilon}^2)\bigg]\cdot p(\beta_1)\cdot...\cdot p(\beta_P)p(u_1)\cdot ... \cdot p(u_J)p(\sigma_{u}^2)p( \sigma_{\varepsilon}^2) 
\end{aligned}
$$

and we employ MCMC using Stan to simulate draws from it. Similarly, the Stan output will be 2000 draws from the approximate joint posterior distribution

$$
\begin{bmatrix}
  \beta_1^{(1)} & \dots & \beta_P^{(1)} & u_1^{(1)} & \dots & u_J^{(1)} & (\sigma_{u}^2)^{(1)} & (\sigma_{\varepsilon}^2)^{(1)} \\
  \\ \vdots &  & \vdots & \vdots & & \vdots & \vdots & \vdots \\ \\
  \beta_1^{(2000)} & \dots & \beta_P^{(2000)} & u_1^{(2000)} & \dots & u_J^{(2000)} & (\sigma_{u}^2)^{(2000)} & (\sigma_{\varepsilon}^2)^{(2000)}
\end{bmatrix}
$$

#### (Generalized) Linear Regression Component: Gamma

An alternative model that we considered in this thesis was a Gamma Generalized Linear Model. The motivation for this was to have a model that is more flexible to the distribution of the non-zero response. Figure \@ref(fig:gamma) displays several versions of a Gamma distribution with various parameters. In particular, note that the Gamma distribution is able to capture the fact that the non-zero response can be skewed in a way that a Normal distribution simply can't.

```{r gamma, echo = F, warning = F, message = F, fig.align = 'center', out.width="80%", fig.cap="The Gamma(a, b) Distribution"}
gamma <- ggplot(data.frame(x = c(0, 20)), aes(x)) +
  stat_function(
    fun = dgamma, args = list(shape = 2, rate = 0.5),
    linewidth = 1.5, aes(color = "one")
  ) +
   stat_function(
    fun = dgamma, args = list(shape = 9.5, rate = 2),
    linewidth = 1.5, aes(color = "two")
  ) +
   stat_function(
    fun = dgamma, args = list(shape = 7.5, rate = 1),
    linewidth = 1.5, aes(color = "three")
  ) +
  scale_color_manual(
    name  = "",
    breaks = c("one", "two", "three"),
    values = c("one" = "#2b66c4", "two" = "#b1cefc", "three" = "#041736"),
    labels = unname(TeX(c("$a = 2, b = 0.5 $", "$a = 9.5, b = 2$", "$a = 7.5, b = 1")))
  ) +
  theme_minimal() +
  labs(
    x = "X",
    y = ""
  ) +
  theme(
    legend.position = c(.7, .8),
    legend.text = element_text(size = 8),
    plot.title = element_text(size = 11.5)
  )

gamma
```


Because the response variable we are working with is strictly positive and often-times right skewed, it's logical to try to model the response as coming from a Gamma distribution. The model is formulated as follows

$$
y_{ij} \sim \text{Gamma}\bigg(\alpha, \frac{\alpha}{\mu_{ij}}\bigg) \qquad \text{where} \qquad \mu_{ij} = \mathbf{x}_{ij}^T\boldsymbol{\beta} + u_j
$$

We are operating under the shape and rate parametrization of a Gamma distribution, and this specific parametrization was chosen so the mean of our response would be the output of the linear component:

$$
E[y_{ij}] = \alpha \bigg(\frac{\alpha}{\mu_{ij}}\bigg)^{-1} = \mu_{ij}
$$

The random intercepts have the same types of prior as in the normal model:

$$
u_j \ | \ \sigma_{u}^2 \sim \mathcal{N}(0, \sigma_{u}^2)
$$

with hyperprior $\sigma_{u}^2$. We then define priors for the other parameters as

$$
\begin{aligned}
\beta_p &\sim \mathcal{N}(m_p, s_p^2)  \qquad \forall p\in P \\
\alpha &\sim \text{Half-Cauchy}(t_1, r_1^2)\\
\sigma_{u}^2 &\sim \text{Half-Cauchy}(0, r_2^2)
\end{aligned}
$$

where $m_p, s_p^2, t_1, r_1, r_2$ are real valued numbers. While the Cauchy prior put on the shape parameter $\alpha$ is still bounded below by zero, we now allow it's mode to be set based on the given data.

Letting $\boldsymbol{\delta} = (\delta_1, ..., \delta_P)$ and $\mathbf{u} = (u_1, ..., u_J)$ we can write out the joint posterior distribution as

$$
\begin{aligned}
p(\boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \alpha \ | \ \mathbf{y}) &=\bigg[\prod_{i:y_{i} > 0}p(y_{i} \ | \ \boldsymbol{\beta},\mathbf{u}, \sigma_{u}^2, \alpha)\bigg]\cdot p(\beta_1)\cdot...\cdot p(\beta_P)p(u_1)\cdot ... \cdot p(u_J)p(\sigma_{u}^2)p( \alpha) 
\end{aligned}
$$

Hopefully at this point the repetitiveness of this process and of these formulations has helped to drill home the Bayesian method of model fitting. We've seen it in three different flavors: Logistic regression, Normal regression, and Gamma regression, but all that's really changed at each step has been the link function and the priors and parameters used.

## Evaluation of the Bayesian Model: Posterior Predictive Distribution

In the Frequentist frame the model parameters are treated as fixed but unknown and so once we obtain estimates for them, we simply use those point estimates to make predictions.

$$
\begin{aligned}
\hat{y}^*_{ij} &= \mathbf{x}_{ij}^T\hat{\boldsymbol{\beta}} + \hat{u}_j \\
\hat{p}_{ij} &= \frac{1}{1 + e^{-(\mathbf{x}_{ij}^T\hat{\boldsymbol{\gamma}} + \hat{v}_j)}} \\
\hat{Y}_{j} &= \sum_{i\in n_j}\hat{y}^*_{ij}\cdot \hat{p}_{ij} 
\end{aligned}
$$

But in the Bayesian frame our model parameters are no longer fixed values, but are described by a posterior distribution. Instead of producing predictions that are single values, we construct what are called posterior predictive distributions. In fact it makes sense why we would end up with predictive distributions rather than single values when you consider that there are two main sources of variability that should be taken into account in our predictions:

1.  Sampling variability in the data: we never expect our model to be perfectly deterministic, rather the real outcomes should be expected to vary around the model.

2.  Posterior variability of the model parameters: we shouldn't go through all the trouble of constructing posterior distributions for our parameters to just throw out that information when it comes time to make predictions, rather we incorporate the variability in our posterior distributions into our predictions.

Within the Bayesian frame, these two sources of variability are combined to produce what is called a posterior predictive distribution.

To get a feel for how this works, I'll just focus on constructing a posterior predictive distribution for a new point $y_{ij, \ new}$ using each model separately.

### Theoretical version

To really stress the logic of what we're doing, imagine that we haven't collected any data yet and that we only had one parameter $A$ in our model. In any model we assume that the data for a fixed parameter $A$ has distribution $p(y \ | \ A)$. Moreover, before having observed any data, all of our uncertainty about the value of $A$ is contained by the prior $p(A)$. We can imagine $p(y \ | \ A)$ capturing the variability in (1) and $p(A)$ capturing the variability in (2).

To produce an estimate for the distribution of a new data point $y_{ij, \ new}$ we simply integrate the product of the previous two terms over $A$.


$$
\begin{aligned}
\int_A p(y_{ij, \ new} \ | \ A)p(A )dA   = p(y_{ij, \ new} )   \\
\end{aligned}
$$

This is sometimes called the prior predictive distribution for $y_{ij, \ new}$ as it represents our knowledge about $y_{ij, \ new}$ before observing any data. 

But we can do much better at describing the variability of the model parameters than this. After observing the sample data we update our knowledge. Again we have the same $p(y_{ij, \ new} \ | \ A)$ which captures the sampling variability in the data, but now the variability of the model parameter is described by the posterior distribution $p(A \ | \ \mathbf{y})$. Again we get a distribution for $y_{ij, \ new}$ by integrating over $A$, except this time we end up with $p(y_{ij, \ new} \ | \ \mathbf{y})$ which is aptly named the **posterior predictive distribution**

$$
\begin{aligned}
\int_A p(y_{ij, \ new} \ | \ A, \mathbf{y})p(A \ | \ \mathbf{y})dA &=  \int_A p(y_{ij, \ new} \ | \ A)p(A \ | \ \mathbf{y})dA \qquad y_{ij, \ new} \ \  \text{independent of } \mathbf{y} \\
&= p(y_{ij, \ new} \ | \ \mathbf{y})\
\end{aligned}
$$

Note that the quality of this posterior predictive distribution depends strongly on the quality of our posterior distribution. In other words, this distribution will only accurately capture the structure of new data points, if the underlying posterior distribution correctly captures the structure of the parameters of interest.

As we move on to the expression for each full model, just remember that while the integral looks very complicated, all that we're doing is incorporating both sources of variability and averaging across the possible values of the model parameters. The posterior predictive distribution for a point $y^*_{ij, \ new}$ using the Normal regression model can be written as:

$$
\begin{aligned}
p(y^*_{ij, \ new} \ | \ \mathbf{y}) &=  \int_{\beta_1}  \dots \int_{\beta_P} \int_{u_1} \dots \int_{u_J} \int_{\sigma_{u}^2} \int_{\sigma_{\varepsilon}^2} \bigg[p(y^*_{ij, \ new} \ | \ \boldsymbol{\beta}, \boldsymbol{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2)p(\boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \sigma_{\varepsilon}^2 \ | \ \mathbf{y})\bigg] \\
& \ \ \ \ \ d\beta_1 ...  d\beta_Pdu_1...du_Jd\sigma_{u}^2d\sigma_{\varepsilon}^2
\end{aligned}
$$

Similarly, for the Gamma model it can be written as

$$
\begin{aligned}
p(y^*_{ij, \ new} \ | \ \mathbf{y}) &=  \int_{\beta_1}  \dots \int_{\beta_P} \int_{u_1} \dots \int_{u_J} \int_{\sigma_{u}^2} \int_{\alpha} \bigg[p(y^*_{ij, \ new} \ | \ \boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \alpha)p(\boldsymbol{\beta}, \mathbf{u}, \sigma_{u}^2, \alpha \ | \ \mathbf{y})\bigg] \\
& \ \ \ \ \ d\beta_1 ...  d\beta_Pdu_1...du_Jd\sigma_{u}^2d\alpha
\end{aligned}
$$

And for the classification model it can be expressed as

$$
\begin{aligned}
p(z_{ij, \ new} \ | \ \mathbf{y}) &=  \int_{\gamma_1}  \dots \int_{\gamma_P} \int_{v_1} \dots \int_{v_J} \int_{\sigma_{v}^2}  \bigg[p(z_{ij, \ new} \ | \ \boldsymbol{\gamma}, \mathbf{v}, \sigma_{v}^2)p(\boldsymbol{\gamma}, \mathbf{v}, \sigma_{v}^2 \ | \ \mathbf{y})\bigg] \\
& \ \ \ \ \ d\gamma_1 ...  d\gamma_Pdv_1...dv_Jd\sigma_{v}^2
\end{aligned}
$$

The distributions $p(y^*_{ij, \ new} \ | \ \mathbf{y})$ and $p(z_{ij, \ new}\ | \ \mathbf{y})$ not only capture where we think each prediction might lie, but also how we would expect it to vary. Often in predictive modeling we're interested in quantifying the uncertainty in our model estimates, and in the Bayesian framework these are baked right into the predictions themselves.

While the theory behind constructing these posterior predictive is pretty intuitive, it's clear that even in the case of a fairly simple model, the actual computations are rather unwieldy. Again, we are saved by the fact that in practice the posterior is too complex to algebraically solve for, so we're already functioning in a setting where we use MCMC to simulate draws from the approximate posterior.

### MCMC version

First I will describe how the posterior predictive distribution is derived from the MCMC draws, and then I will explain how it approximates the exact calculation above.

To generate a posterior predictive distribution for a new data point $y_{ij,\ new}$ using the normal regression model we simulate a prediction from the model for each parameter set of the MCMC output.

$$
\begin{bmatrix}
y_{ij, \ new}^{(1)} \sim \mathcal{N}\Big( \mathbf{x}_{ij, \ new}^T \boldsymbol{\beta}^{(1)} + u_j^{(1)}, (\sigma_{\varepsilon}^2)^{(1)}\Big) \\
\\
\vdots \\
\\
y_{ij, \ new}^{(2000)} \sim \mathcal{N}\Big(\mathbf{x}_{ij, \ new}^T \boldsymbol{\beta}^{(2000)} + u_j^{(2000)}, (\sigma_{\varepsilon}^2)^{(2000)}\Big)
\end{bmatrix}
$$

The result is a set $\Big\{y_{ij , \ new}^{(1)}, y_{ij, \ new}^{(2)}, ..., y_{ij, \ new}^{(2000)}\Big\}$ which approximates the posterior predictive distribution.

For the Gamma model we do the same thing but using the appropriate distribution

$$
\begin{bmatrix}
y_{ij, \ new}^{(1)} \sim \text{Gamma}\Bigg(\alpha^{(1)}, \frac{\alpha^{(1)}}{ \mathbf{x}_{ij, \ new}^T\cdot \boldsymbol{\beta}^{(1)} + u_j^{(1)}}\Bigg) \\
\\
\vdots \\
\\
y_{ij, \ new}^{(2000)} \sim \text{Gamma}\Bigg(\alpha^{(2000)}, \frac{\alpha^{(2000)}}{ \mathbf{x}_{ij, \ new}^T\cdot \boldsymbol{\beta}^{(2000)} + u_j^{(2000)}}\Bigg)
\end{bmatrix}
$$

And we do the same thing for the classification model

$$
\begin{bmatrix}
z_{ij, \ new}^{(1)} \sim \text{Bernoulli}\Bigg(\frac{1}{1 + e^{-\big(\mathbf{x}_{ij}^T\boldsymbol{\gamma}^{(1)} + v_j^{(1)}\big)}}\Bigg) \\
\\ \vdots \\ \\
z_{ij, \ new}^{(2000)} \sim \text{Bernoulli}\Bigg(\frac{1}{1 + e^{-\big(\mathbf{x}_{ij}^T\boldsymbol{\gamma}^{(2000)} + v_j^{(2000)}\big)}}\Bigg)
\end{bmatrix}
$$

While it may not be immediately clear, these processes are really just mimicking what the massive integrals above were computing exactly. By simulating realizations of the distribution behind each model, we are again capturing the sampling variability in the data, and by doing so across all of our MCMC parameter draws, the uncertainty about the model parameters is being incorporated as well.

### Combining the Model Predictions

Now that we have two sets which represent approximate the posterior predictive distribution for unit $i$ in group $j$ for each respective model, we have to think about how we combine them. After all, our final model prediction is the product of these two models, So we certainly need a posterior predictive distribution of $y_{ij, \ new} = y_{ij, \ new}^*p_{ij, \ new}$, but it's unclear how we should combine the predictive distributions from the individual models to get here. In the Frequentist version where our predictions are single point values, this poses no problem at all, but now that our predictions are themselves distributions, it's a little less clear how to proceed. We might just match MCMC iteration $k$ from each model together, but what makes this matching more correct than shuffling the iterations and then matching them up?

One solution to this conundrum of combining the distributions is to simply build the models simultaneously. In practice this relies on a few tricks and definitely increases the complexity when actually writing code for it, but it can be done and it does allow us to avoid this problem. That being said, as the two models grow to be more complicated, this process of building them simultaneously grows much more difficult and so it isn't a very robust solution to the problem. In the next section we walk through a nice theoretical finding that offers a solution to this problem.

## Simultaneous v.s Separate

### Simultaneous Model Build

As we just mentioned, to get around our problem of how we combine the MCMC iterations for the models built separately, we could fit the models simultaneously. The one major assumption that we will have here is that there is no dependence in the priors **between** models. While there are certainly cases where this doesn't hold, trying to incorporate these dependencies into the model incorporate a lot more complexity without much performance gain^[@pfeffermann2008small].

Finally, this result holds regardless of the particular models that we use, but for the sake of simplicity we'll use a logistic regression model with no random effects and a Normal linear regression model with no random effects. As mentioned above, we have to get a bit crafty when building the models simultaneously and the way we do this is as follows. First we fit the logistic regression model as we normally would:

$$
z_{ij} \sim \text{Bernoulli}\Bigg(\frac{1}{1 + e^{-\mu_{ij}}}\Bigg) \qquad \text{where} \qquad \mu_{ij} =\mathbf{x}_{ij}^T\boldsymbol{\gamma} + v_j
$$

But then the two part process gets folded into the way that we set up the Normal linear regression model: 

$$
y_{ij} \sim \mathcal{N}\bigg(z_{ij}\cdot m_{ij} \ , \ \Big[z_{ij}\cdot\tau_1 + (1 - z_{ij})\tau_2\Big]\bigg) \qquad \text{where} \qquad m_{ij} =\mathbf{x}_{ij}^T\boldsymbol{\beta} + u_j
$$

Here $\tau_1$ represents the variance of $y_{ij}$ when $z_{ij} = 1$ and thus can be estimated, but $\tau_2$ is the variance of $y_{ij}$ when $z_{ij} = 0$ and so there's no practical way for it to be estimated by the MCMC algorithm. The solution is to just set $\tau_2$ as some small fixed number (i.e $0.001$) and approximate the posterior for all the other parameters.

With all of that in mind, in this setting our posterior for both models would be:

$$
\begin{aligned}
p(\boldsymbol{\beta}, \boldsymbol{\gamma}, \mathbf{u}, \mathbf{v}, \sigma_u^2, \sigma_v^2, \tau_1\ | \ \mathbf{y}) &\propto p(\mathbf{y} \ | \boldsymbol{\beta}, \boldsymbol{\gamma}, \mathbf{u}, \mathbf{v}, \sigma_u^2, \sigma_v^2, \tau_1)p(\boldsymbol{\beta}, \boldsymbol{\gamma}, \mathbf{u}, \mathbf{v}, \sigma_u^2, \sigma_v^2, \tau_1) 
\end{aligned}
$$

We can expand this by writing out the likelihood more fully based on whether $y$ is zero or not:

$$
\begin{aligned}
p(\boldsymbol{\beta}, \boldsymbol{\gamma}, \mathbf{u}, \mathbf{v}, \sigma_u^2, \sigma_v^2, \tau_1 \ | \ \mathbf{y}) & \propto \bigg[\prod_{i:y_i = 0}(1-p_i)\prod_{i:y_i > 0}(p_i)p(y_i \ | \ \boldsymbol{\beta}, \mathbf{u}, \sigma_u^2, \tau_1)\bigg]\cdot  p(\boldsymbol{\beta}, \boldsymbol{\gamma}, \mathbf{u}, \mathbf{v}, \sigma_u^2, \sigma_v^2, \tau_1)\\
\end{aligned}
$$

While it wasn't too difficult to write this out up to a proportionality constant, in practice it can be very difficult to figure out how to combine the two models in such a way that the MCMC algorithm still converges once you start using models that are more complicated than these ones.

But, there's important insight still to be found here. Let's group these terms based on the parameters that they use. In particular we'll group by which individual model the parameter belongs to:

$$
\begin{aligned}
&= \Bigg[\Big(\prod_{i:y_i = 0}(1- p_i)\prod_{i: y_i > 0}p_i\Big)p(\boldsymbol{\gamma}, \mathbf{v}, \sigma_v^2)\Bigg]\Bigg[\Big(\prod_{i:y_i > 0}p(y_i \ | \ \boldsymbol{\beta}, \mathbf{u}, \sigma_u^2, \tau_1)\Big)p(\boldsymbol{\beta},\mathbf{u}, \sigma_u^2, \tau_1)\Bigg]
\end{aligned}
$$

Again, we are able to split the joint prior in this way because we are assuming that there is no dependence in the priors **between** models.

But now, if we look at this closely we can see that what we really have here is a full separation into the posteriors for the individual models for $p$ and $y^*$ as seen in our derivation in the previous section. This means that we can write:

$$
\begin{aligned}
p(\boldsymbol{\beta}, \boldsymbol{\gamma}, \mathbf{u}, \mathbf{v}, \sigma_u^2, \sigma_v^2, \tau_1\ | \ \mathbf{y})  &\propto p(\boldsymbol{\beta}, \mathbf{u}, \sigma_u^2, \tau_1 \ | \ \mathbf{y})p(\boldsymbol{\gamma}, \mathbf{v}, \sigma_v^2 \ | \ \mathbf{y}) \\
  &= C\bigg[p(\boldsymbol{\beta}, \mathbf{u}, \sigma_u^2, \tau_1 \ | \ \mathbf{y})p(\boldsymbol{\gamma}, \mathbf{v}, \sigma_v^2 \ | \ \mathbf{y})\bigg]
\end{aligned}
$$

Finally, since these are all proper probability distributions we know that they should all integrate to 1 when integrated across all of their parameters. If we integrate both sides over all of the parameters from both models, its clear that the LHS would be one, and once we recall that there is no parameter dependence **between** the models, it is clear that the RHS does as well. And so we are left with the conclusion that $C = 1$ and thus that

$$
p(\boldsymbol{\beta}, \boldsymbol{\gamma}, \mathbf{u}, \mathbf{v}, \sigma_u^2, \sigma_v^2, \tau_1\ | \ \mathbf{y})  = p(\boldsymbol{\beta}, \mathbf{u}, \sigma_u^2, \tau_1 \ | \ \mathbf{y})p(\boldsymbol{\gamma}, \mathbf{v}, \sigma_v^2 \ | \ \mathbf{y}) 
$$

So the full posterior for the model built simultaneously is equal to the product of the posteriors for each model built separately. The major upshot here is that we can fit the models separately, and then combine the results at the end to get our posterior predictive distributions. In other words *as long as we don't build in any correlations between the parameters in the two models, then we can build each model as complex as we might desire without having to worry about how we will eventually build the two models together. As we learned above, we can simply build them separately and combine the results at the end.*. This is a really nice theoretical finding as it alleviates the need to figure out how to build the models simultaneously and encourages us to have freedom in how we build each individual model.

### Practical Backing

To test our theoretical work above, we fit simpler versions of the two models with no random effects. And indeed, when we fit the models simultaneously and also fit them separately making use of MCMC to simulate samples from their approximate posteriors, we find that they are practically identical.

```{r, echo=F, message=F, warning=F, fig.align='center', out.width='80%'}
library(here)
full_comp <- read_csv(here("data", "sep_sim_comp.csv"))

full_comp %>% 
  ggplot(aes(x = value, fill = model)) +
  geom_density(alpha = 0.9, color = NA) +
  scale_fill_manual(values = c("#ffb703", "#023047")) +
  facet_wrap(~param, scales = "free") +
  theme_minimal() +
  labs(fill = "Model") +
  theme(legend.position = "bottom")
```

## Posterior Predictive Distribution Finalized

Now that we know that we can fit the models separately and then combine them at the end, we are finally ready to describe how final predictions are made.

Since we are interested in a making predictions for the average response in group $j$ we obtain a set that approximates the posterior predictive distribution for each unit $i$ in group $j$. For example, if we fix $j$ and let $n_j = 5$ we would have 5 sets:

$$
\begin{aligned}
&\Big\{y_{1j , \ new}^{(1)}, \ y_{1j, \ new}^{(2)}, ..., \ y_{1j, \ new}^{(2000)}\Big\} \\
&\Big\{y_{2j , \ new}^{(1)}, \ y_{2j, \ new}^{(2)}, ...,  \ y_{2j, \ new}^{(2000)}\Big\} \\
& \qquad \qquad \qquad \ \   \vdots \\
&\Big\{y_{5j , \ new}^{(1)}, \ y_{5j, \ new}^{(2)}, ..., \ y_{5j, \ new}^{(2000)}\Big\}
\end{aligned}
$$

To get the posterior predictive distribution for $\bar{y}_{j, \ new}$ we take averages across units in group $j$ by indices of the MCMC draws. In other words we take the average of $\Big\{y_{1j, \ new}^{(k)}, y_{2j, \ new}^{(k)}, ... , y_{5j, \ new}^{(k)}\Big\}$ for each MCMC draw $k$. In full we end up with the set:

$$
\begin{bmatrix}
\frac{1}{n_j} \sum_{i = 1}^{n_j}\hat{y}_{ij, \ new}^{(1)} \\
\frac{1}{n_j} \sum_{i = 1}^{n_j}\hat{y}_{ij, \ new}^{(2)} \\
\vdots \\
\frac{1}{n_j} \sum_{i = 1}^{n_j}\hat{y}_{ij, \ new}^{(2000)}
\end{bmatrix} \qquad = \qquad 
\begin{bmatrix}
\hat{Y}_{j, \ new}^{(1)} \\
\hat{Y}_{j, \ new}^{(2)} \\
\vdots \\
\hat{Y}_{j, \ new}^{(2000)}
\end{bmatrix}
$$

Which is an approximation of the posterior predictive distribution for the mean of the response in group $j$.


## Prediction Intervals

In statistical modeling, another piece of information that we're often interested is a measure of our uncertainty in our predicted values. Often times these are referred to broadly as prediction intervals, and whereas the confidence intervals and credible intervals that we described in Chapter \@ref(bayes-freq) provide uncertainty bounds for parameter estimates, these prediction intervals provide uncertainty bounds for a future observation or data point.

Immediately, we can see how straightforward it is to acquire these in the Bayesian setting. Because our predictions are not simply point estimates, but are predictions themselves, these prediction intervals are baked right into the prediction process. 

Unfortunately it is far less straightforward in the Frequentist setting. While prediction intervals are straightforward to generate for singular regression models, things get a lot more complicated with a two part model like ours. The reality is that we spent a good amount of time trying to construct a bootstrap procedure to generate these prediction intervals and beyond being very computationally intensive, we also couldn't find one that actually worked correctly. While this was certainly frustrating, the struggle to try to develop a process for generating these prediction intervals for the Frequentist models, really highlighted how nice it is that you get them for free in the Bayesian model.

#### A note of caution

While it might be tempting to gleefully pronounce Bayesian models to be better than Frequentist simply for the ease of access to prediction intervals, the important caveat is that these prediction intervals will only be correct when the Bayesian model is correct. What I mean by that is that if our Bayesian model badly fits the data, then we still get uncertainty estiamtes for free, but a 95% prediction interval will likely not get 95% coverage, thus indicating that the intervals themselves are also incorrect. 

