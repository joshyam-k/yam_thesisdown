# Introduction {#intro-section}

In this thesis, I will concerned with modeling a response variable from various explanatory variables in data that exhibits two distinctive features: (i) the response variable that is "zero-inflated", and (ii) the data is "clustered".

## Zero-Inflated Data

As the name suggests, data are canonically classified as being zero-inflated when they contain a significant proportion of zeroes. While it's hardly ever very productive to spell out a definition for a phrase that is its own definition, I do so here to emphasize the fact that to call data zero-inflated is to only say something very broad about how that data is distributed. There is no commonly accepted cutoff for at what proportion of zeros our data deserves the label zero-inflation, and there is no restriction on the distribution of the non-zero data. While the work done in this thesis concerns zero-inflated data with no constraint on the level of "zeroness", I do require that the non-zero data is positive and continuously distributed. For example, the response variable might look like this:

```{r ziex, echo=F, message=F, warning=F, fig.align='center', out.width='85%', fig.cap="Zero-Inflated data"}
set.seed(6)
library(tidyverse)
library(here)
library(gt)
library(patchwork)

z <- tibble(
  z = rep(0, 2500)
)

nz <- tibble(
  nz = rgamma(1000, 3, 4)
)

ggplot() +
  geom_density(
    data = filter(nz, nz > 0.25),
    aes(x = nz, y = ..count..),
    fill = "#023047",
    color = "white",
    alpha = 0.9
    ) +
  geom_histogram(
    data = z,
    aes(x = z),
    fill = "#023047",
    color = "white",
    alpha = 0.9
    ) +
  labs(
    x = "Response Variable"
  ) +
  theme_bw()
```

And in reality, this form of zero-inflated data is one that we see quite often in the real world. Importantly, an abundance of zeros in a measured variable might come about for a variety of different reasons. Sometimes it could be a characteristic of the data itself, for example if we collected data on the total weight of fish caught at a lake by individuals on a given day, we likely would see a lot of individuals who caught zero fish leading to a significant portion of zeros in our data. But other times it could be a characteristic of the data collection process itself, for example a measurement error or a sampling error could cause data to be zero-inflated as well.

Importantly, as I am working in a modeling setting, *when I say that data is zero-inflated I mean that the response variable is zero-inflated*. Although I will simply refer to my data as being zero-inflated and my models as being suited for zero-inflated data for the duration of this thesis, this is simply a matter of convenience and not a statement that the methods work for any situation in which data can be considered zero-inflated. Put simply, *I explore, present, and evaluate a model that is suited for a response variable which has a significant portion of zeroes, with the non-zero portion of that variable belonging to a positive continuous distribution.*

## Clustered Data

Furthermore, we will be operating in a setting where the data is not only zero-inflated, but where it also exhibits a clustered structure. Again, the notion of data being clustered is a very non precise one. For this thesis we will not put a very strong restriction on what this looks like. Our data will be clustered in the sense that there is meaningful grouping in the data structure that makes data points within the same cluster more alike on average than points across clusters.

For example, going back to the fishing in a lake example. If we looked at data on the weight of each individual fish caught, we would imagine that fish of the same species would generally be more similar in weight than two fish from different species.


## Forestry Setting

One specific setting that exhibits both of these data features is data on the United States' forests. In particular, I will focus on forestry data collected by the Forestry Inventory & Analysis Program (FIA) of the U.S. Government. The FIA monitors the nation's forests by collecting data on, and providing estimates for, a wide array of forest attributes. Not only is this work vitally important, but it's essential that it be done accurately and efficiently: "The FIA is responsible for reporting on dozens, if not hundreds, of forest attributes relating to merchantable timber and other wood products, fuels and potential fire hazard, condition of wildlife habitats, risk associated with fire, insects or disease, biomass, carbon storage, forest health, and other general characteristics of forest ecosystems."[@mcconville2020tutorial].

The FIA employs a quasi-systematic sampling design to determine which locations to physically collect forest attribute data at. These sampled locations are referred to as plot-level data and the FIA sends a crew out to physically measure a wealth of forest attributes at that location. As you might expect, not only is this method extremely time intensive, but it is also very expensive. The vastness of the nation's forests in tandem with the resources needed to collect plot-level data, make it impossible to collect census level data on forest metrics. Thus, the need for additional data sources as well as statistical models are vital to the work that the FIA does. The main secondary data source that the FIA employes as covariates in their statistical models is remote sensed data. The remote sensed data typically includes climate metrics (e.g. temperature and precipitation), geomorphological measures (e.g. elevation and eastness), as well as metrics like proportion of forest which can be measured from a satellite.

While the main use of the additional remote sensed data sources are to increase the accuracy of the estimators that the FIA builds, they are also used to make rational decisions about the aforementioned plot-level data collection. Before sending a crew out to a given sampled location, the FIA will first look at the remote sensed data for that location. If that location happens to be in a place where there is clearly no forest, for example in the middle of a parking lot, the FIA will not send a crew out and instead will mark all forest attributes for that location as being zero. As you might imagine, this happens quite a bit, and so an interesting characteristic of many forest attribute variables collected by the FIA is that they are zero-inflated. Importantly, this is an example of where the data is zero-inflated because of the data collection process.

If we look at the distribution of the FIA collected forest attribute "Dry Above Ground Biomass From live Trees", we can see that it is indeed quite zero-inflated.

```{r forestrydata, echo = F, message=F, warning = F, cache=T, fig.align='center', out.width='80%', fig.cap='Zero-Inflated Forestry data.'}
data <- readRDS(here("data", "wa_plots_public.rds"))
ggplot() +
  geom_density(
    data = data[data$DRYBIO_AG_TPA_live_ADJ > 25, ],
    aes(x = DRYBIO_AG_TPA_live_ADJ, y = 20*..count..),
    fill = "#023047",
    color = "white",
    alpha = 0.9
    ) + 
  geom_histogram(
    data = data[data$DRYBIO_AG_TPA_live_ADJ == 0, ],
    aes(x = DRYBIO_AG_TPA_live_ADJ),
    bins = 40,
    fill = "#023047",
    color = "white",
    alpha = 0.9
    ) +
  labs(
    x = "Dry Above Ground Biomass from Live Trees",
    y = "Count"
  ) +
  theme_bw()
```

Importantly, the FIA groups the continental U.S. into smaller domains called Eco-Subsections. These Eco-Subsections are drawn with the goal of maintaining internal ecologically homogeneity as best as possible. Thus each data point belongs to a specific Eco-Subsection and it's this grouping that gives us a clustered data structure.

Not only is the FIA data a good real life example of when we see this zero-inflated and clustered data structure, but it's also a setting in which it's quite important that the models used to estimate these forest attributes are sufficiently accurate and efficient.

###  Immediate Modeling Struggles

To motivate using a more complex method, I'll first show what happens when I try to just fit a simple linear regression to this type of data. If we regress our response variable on a useful covariate and plot both the data and the simple linear regression line together we get the following

```{r scatterzi, echo=F, message=F, warning=F, cache=T, fig.align='center', out.width='80%', fig.cap="The shortcomings of a simple linear regression model in this context."}
set.seed(30)
data %>% 
  slice_sample(n = 1500) %>% 
  ggplot(aes(x = tcc, y = DRYBIO_AG_TPA_live_ADJ)) +
  geom_point(alpha = 0.6, size = 2) +
   stat_smooth(
    geom = "line", method = "lm",
    se = F, alpha = 0.9,
    linewidth = 2, color = "#219ebc"
    ) +
  labs(
    x = "Tree Canopy Coverage",
    y = "Dry Above Ground Biomass From live Trees"
  ) +
  theme_bw()
```

While this model isn't awful it's certainly misspecified. What I mean by this is that a simple straight line shown in Figure \@ref(fig:scatterzi) doesn't appropriately capture the dynamics of the relationship between our covariate and our response. The zero-inflation in the response variable pulls the regression line down so that it doesn't properly capture the relationship between the explanatory variable and the *non-zero* response, but more importantly it doesn't capture the structure of zeros in the response at all. We can see that the only time this model will predict a near zero response is when the covariate value is very close to zero, but this is an extreme limitation of the model since we observe zero response values across almost the entire range observed values for the covariate. What's more, a simple linear regression model does not allow us to understand how the probability our response variable being zero, changes with our covariate.

We would call this model statistically biased, as it is overly simple and thus doesn't properly capture the structure of the data. While it's perhaps feels unfair to motivate my method by piting it against the simplest of statistical models, the reality is that linear regression is a very powerful and widely used model. Moreover, in a setting such as this one where the data looks plausibly linear, the principle of parsimony might make a linear regression model a well reasoned choice. While there's certainly a need for a model that is better fit to the data, I won't go down the route of constructing an incredibly opaque and complex deep learning model to do so. Instead they model I present is interpretable and intuitive while being flexible enough to capture the structure of the zero-inflated data.

### The New Model

While I will exhaustively describe details of, and the math behind, the exact model in a later section, I'll go through a non-technical overview of how it will function here.

The defining characteristic of the model is that it is a two-part model. Instead of trying to fit the data with a singular model, we instead fit two different models and then combine them at the end. The two models are

1.  A classification model fit to the entire data set that predicts how likely it is that a certain data point has a non-zero response value.

2.  A regression model fit to the *non-zero* portion of the dataset that predicts the continuous response variable.

To get a final prediction for a data point we take the prediction from model (1) and multiply it by the prediction from model (2).

$$
\text{Final Prediction} = \underbrace{\bigg(\text{Regression Model Output}\bigg)}_{\text{Model (2)}} \times \underbrace{\bigg(\text{Classification Model Output}\bigg)}_{\text{Model (1)}}
$$

Let's think back to Figure \@ref(fig:scatterzi) to unpack why this is a reasonable strategy.

```{r modintuition, echo=F, message=F, warning=F, fig.align='center', out.width='95%', fig.height=3.6, fig.cap="Example of two new data points that we might be interested in testing our model on."}
set.seed(19)
data_samp <- data %>% 
  slice_sample(n = 1000) %>% 
  filter(DRYBIO_AG_TPA_live_ADJ < 250) %>% 
  mutate(ratio = DRYBIO_AG_TPA_live_ADJ/tcc) %>% 
  filter(ratio > 0.1 | ratio == 0) 

p1 <- data_samp %>% 
  ggplot(aes(x = tcc, y = DRYBIO_AG_TPA_live_ADJ)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_point(data = tibble(tcc = 50, DRYBIO_AG_TPA_live_ADJ = 0),
             aes(x = tcc, y = DRYBIO_AG_TPA_live_ADJ), color = "red", size = 5) +
  labs(
    x = "Tree Canopy Coverage",
    y = "Dry Above Ground Biomass From live Trees"
  ) +
  ylim(c(0, 280)) +
  theme_bw()

p2 <- data_samp %>% 
  ggplot(aes(x = tcc, y = DRYBIO_AG_TPA_live_ADJ)) +
  geom_point(alpha = 0.6, size = 2) +
  geom_point(data = tibble(tcc = 50, DRYBIO_AG_TPA_live_ADJ = 70),
             aes(x = tcc, y = DRYBIO_AG_TPA_live_ADJ), color = "red", size = 5) +
  labs(
    x = "Tree Canopy Coverage",
    y = ""
  ) +
  ylim(c(0, 280)) +
  theme_bw()

p1 + p2

```

Let's start with the left hand side of Figure \@ref(fig:modintuition) that has a data point with zero response value. We'd expect a very bad prediction from model (2) which was only fit to the non-zero data, but the intuition here is that if our classification model is sufficiently accurate then it shouldn't matter, as it will identify the point as being zero and thus the prediction will just be sent close to zero by that classification model output.

On the other hand, the right hand side of Figure \@ref(fig:modintuition) has a data point with non-zero response value. We'd expect a good prediction from model (2) here and again if the classification model is sufficiently accurate then it should identify it as being a likely non-zero data point and the model (2) prediction should be relatively unchanged when multiplied by the classification model output.

Clearly the success of this strategy relies on the quality of the two individual models, but at least we can see how this method has the capacity to capture the zero-inflation data structure.


### Building the Model

Now, as the title suggests I'll be building these models in a Bayesian frame. But what does that even mean and why would one want to do that? While most of the thesis will be devoted to answering the second question, I'll spend some time in the next section describing Bayesian methods, and walking through how they differ from a Frequentist approach.

Importantly, since this thesis is simply an earnest exploration of a Bayesian method, it has no intention to participate in the deep and opaque philosophical dialogue regarding whether Bayesian or Frequentist methods are a more "correct" way to do statistics.

That being said, the word Bayesian is so overwhelmingly ideologically tied to this statistical dichotomy that it is, by nature, very difficult to talk about a Bayesian method without talking about Frequentism as well. Because classical statistical methods are all Frequentist ones, there is often a pressure to validate a Bayesian method by standing it next to its Frequentist counterpart. While this Bayesian thesis will indeed feature an alternative Frequentist method, it does so, not to argue for one side or the other, but rather to illustrate some of the key differences in, and logic behind, Bayesian and Frequentist analyses.

## Looking Ahead

In order to introduce, study, and implement these models I will structure the research in the following way:

-   Chapter \@ref(bayes-freq) gives a thorough functional overview of how Bayesian and Frequentist methods differ in the simple setting of inference for a mean. The goal here is primarily to provide a gentle introduction to Bayesian data analysis, so as not to drop the reader into the deep end when the main model is introduced.

-   Chapter \@ref(methods-sec) gives a detailed overview of all the methods employed in the thesis. It starts with a high-level description of the Cluster Zero-Inflation model, before moving on to detailed descriptions of how each model will be built. Next, prediction for Bayesian models is illustrated both theoretically and computationally. 

- Chapter \@ref(semsim) walks through a proof that provides justification for building each part of the Bayesian two part model separately and then combining the results at the end.

-   Chapter \@ref(sim-study) sets up the simulation study that serves as the main process by which we evaluate the various models.

-   Chapter \@ref(res-sec) showcases the results of each model's performance in the simulation study. Beyond comparing the performance metrics of each model, I also describe the challenges associated with making comparisons between Bayesian and Frequentist models in a complex setting like this one.

-   Chapter \@ref(vignette) gives an overview of the R Package written to accompany the methods explored in this thesis. A vignette is provided the applies the R Package to the Forestry data setting.

- Finally, Chapter @\ref(conc) provides some concluding thoughts and describes possible extensions.





