# Package Vignette {#vignette}

## Forestry Data

```{r, message=F, warning=F, echo= F}
library(here)
library(tidyverse)
library(gt)
data <- readRDS(here("data", "wa_plots_public.rds"))
```

In order to show off the functionality of the package that we wrote, we'll apply it to the Forestry data that we introduced all the way back in Chapter \@ref(intro-section). Recall that this forestry data is both zero-inflated and clustered making it a perfect setting for us to employ our models.

The particular subset of Forestry data that we'll be looking at comes from the State of Washington and contains `r nrow(data)` observations across 49 Eco-Subsections (which define our clusters). The summary statistics of the number of observations per group are as follows:

```{r, message=F, warning=F, echo=F}
summary(data %>% count(ECOSUBCD) %>% pull(n)) 
```

The covariate that we will be using from the forestry data is called "Tree Canopy Cover" (tcc) which is defined as the proportion of forest floor covered by a vertical projection of the tree crowns ^[@toney2008stem]. This is one of the FIA's remote sensed variables, so the information is associated with the pixels that overlay the individual plot.

The response variable that we will be using is called "Dry Above Ground Biomass from Live Trees" and is expressed in the units of per area density. This Dry Above Ground Biomass variable is one that is very important for understanding the health of forest ecosystems and plays a role in decisions regarding merchantable timber and fire hazard. For these reasons its vitally important for the FIA to be able to produce accurate estimates of this variable. 

The zero-inflation models presented in this thesis provide a reasonable approach for trying to do exactly that. That being said, no matter how good a method is, it can't be used if the tooling doesn't exist to actually perform it. Thus, the last portion of this thesis will be devoted to describing the very early stages of an R software package that we have written to build and utilize these zero-inflation models.


## R Package: zinf

We start by installing the developmental version of the package and loading it into our R session.

```{r, message=F, warning=F}
devtools::install_github("joshyam-k/zinf")
library(zinf)
```


The two main functions that `zinf` contains are `zinf_bayes` and `zinf_freq` which as the names suggest relate to the Frequentist and Bayesian versions of the zero-inflation models.

The challenge of writing a software package for these zero-inflation models was truthfully a lot trickier than I initially thought it would be. There were two different possible structural versions that I considered to be viable:

1. Write a function that takes as argument all the individual pieces needed to build each of the two models and internally build the two models and then define all of the neccessary functionality.
2. Write a function that takes as argument each of the two already built models and internally combine the models into a single object and define all of the neccessary functionality.

On the one hand, version (1) is nice in that it "does it all" for the user. But the main problem with version (1) is that the functions `lme4::lmer` and `lme4::glmer` for Frequentist models and `rstanarm::stan_lmer` and `rstanarm::stan_glmer` for Bayesian models are already so popular and well developed and robust that it seems silly to to rely on that pre-existing software. After all, as we showed in Section \@ref(sepsim), the Bayesian models can be built separately and made as complex desired, and this is also certainly the case for the Frequentist models. So, while there's an argument to be made for the abstraction of all model building processes, the reality is that under version (1) if I wanted my package to be as familiar and robust as the pre-existing software for building the individual models, I would have to essentially take as arguments every possible function arguments that those `lme4` and `rstanarm` functions take, and internally feed them back into those functions. At that point the question becomes: why not just have the user build the individual models separately and then feed those finished models into our function? And that lands us directly at version (2).

```{r tbl2, echo=F, warning=F, message=F, fig.cap="Two Software Versions", out.width="90%", fig.align="center"}
include_graphics(path = "figure/v1v2.png")
```


In the Frequentist setting, version (1) doesn't seem that bad, especially since most of the time a user only specifies the model formula and the data set within the `lme4` functions, but things grow a lot worse in the bayesian versions where all kinds of priors and MCMC settings need to be specified. In the end, I decided that it made more sense to build version (2). While this does leave more work for the user than I would have liked, I think that the `zinf` functions still have enormous usefullness in that they define methods for producing a summary of the zero-inflation model and most importantly for predicting with the zero-inflation model.


## Application

We'll start by setting aside a small amount of the data set to make predictions on. In particular, Ecosubsections M242Ba and M242Db have quite a few data points so we'll sample 100 points from each one to test our model on. The only other data preparation we need to do is to create a training data set that only contains training data points with a non-zero response value.

```{r, message=F, warning=F}
set.seed(5)
test <- data %>% 
  filter(ECOSUBCD %in% c("M242Ba", "M242Db")) %>% 
  group_by(ECOSUBCD) %>% 
  slice_sample(n = 100) %>% 
  ungroup()

train <- anti_join(data, test)

nonzero_train <- train %>% 
  filter(DRYBIO_AG_TPA_live_ADJ > 0)
```


### Individual Frequentist models

We'll start by building the individual Frequentist models.

```{r, message=F, warning = F}
library(lme4)

freq_reg <- lmer(
  DRYBIO_AG_TPA_live_ADJ ~ tcc + (1 | ECOSUBCD),
  data = nonzero_train
  )

freq_log_reg <- glmer(
  DRYBIO_AG_TPA_live_ADJ != 0 ~ tcc + (1 | ECOSUBCD),
  family = binomial,
  data = train
  )
```


The key characteristics of these models are the specification of the covariates and the response, the random intercept term for the Ecosubsection (ECOSUBCD), and the use of the non-zero data set to fit the first model.

### Individual bayesian models

#### Logistic Regression Model

Next we'll build the individual Bayesian models. We'll start with the logistic regression model that requires priors for 

- The fixed effect slope (for tcc)
- The fixed effect intercept
- The variance of the random intercepts

The prior fixed effect slope should reflect our vague sense for how the probability of Dry Above Ground Biomass from Live Trees being non-zero changes with Tree Canopy Cover. With limited knowledge of how forest attributes interact we'll assume that the slope is positive and centered somewhere around 0.5, so we'll use a prior of $\mathcal{N}(0.5, 0.25^2)$.

The fixed effect intercept that `rstanarm::stan_glmer` asks for is actually the centered intercept. This can be conceptualized as our guess for the probability of being non-zero for an average plot. Given that the response variable is zero in around 40% of the plots in our data set we'll guess that this probability is around 0.4. We have to be careful here, since logistic regression employs a logit link, we actually want to set the prior mean to be on the log-odds scale, so we use $\log{(0.4/(1-0.4))} = -0.4$ as the center. We might guess that the range of the propbability of being non-zero for an average plot might range from 0.1 to 0.7 which translates to a log odds range of $(-2.2, 0.85)$. Thus the prior for the centered fixed effect intercept will thus be $\mathcal{N}(-0.4, 0.7^2)$.

Finally, the variance of the random intercepts is not really a parameter that we can use the data to get a great guess for, so we'll utilize a weakly informative prior of Exponential$(1)$. In `rstanarm::stan_glmer` this is written slightly differently using a function called `decov` but in this setting that is just the Exponential$(1)$ prior that we want.

We'll stick with the default MCMC settings and build the model as follows

```{r, warning = F, message=F, results='hide', cache = T}
library(rstanarm)
bayes_log_reg <- stan_glmer(
  DRYBIO_AG_TPA_live_ADJ > 0 ~ tcc + (1 | ECOSUBCD),
  data = train,
  family = binomial,
  prior = normal(0.5, 0.25),
  prior_intercept = normal(-0.4, 0.7^2),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 2000, seed = 84735,
  cores = parallel::detectCores()
)
```


#### GLM 1: Normal with Identity link

We now turn to the two versions of the generalized linear model. We'll start with the model that uses a Normal distribution and the identity link- i.e Normal linear regression. This time the parameters that need priors are all the same as from the previous one, except now we also need a prior for the variance of the error term.

We can examine a scatter plot of Tree Canopy Cover on Dry Above Ground Biomass from Live Trees to choose our prior for the fixed effect slope. We can do some rise-over-run calculations to end up with a guess that the fixed effect slope is around 2 and probably doesn't go outside of the range (0.5, 5). Thus we'll use a $\mathcal{N}(2, 1^2)$ prior here.

```{r nzdata, echo=F, message=F, warning=F, fig.align='center', out.width='90%', fig.cap="Relationship between Tree Canopy Cover and Dry Above Ground Biomass from Live Trees"}
nonzero_train %>% 
  mutate(ratio = floor(DRYBIO_AG_TPA_live_ADJ/tcc)) %>% 
  filter(ratio > 0) %>% 
  mutate(DRYBIO_AG_TPA_live_ADJ = DRYBIO_AG_TPA_live_ADJ + rnorm(1752, 0, 20)) %>% 
  filter(DRYBIO_AG_TPA_live_ADJ > 0) %>% 
  ggplot(aes(x = tcc, y = DRYBIO_AG_TPA_live_ADJ)) +
  geom_point(alpha = 0.5, size = 2) +
  theme_bw() +
  labs(
    x = "Tree Canopy Cover",
    y = "Dry Above Ground Biomass from Live Trees"
  )
```


Again we can refer back to Figure \@ref(fig:nzdata) to get our guess for the the centered intercept. At an average value of tcc (say 50), the value of the response looks to be around 100, but this could plausibly range anywhere between 20 and 180, so we'll use a $\mathcal{N}(100, 40^2)$ prior for the centered intercept.

Finally we'll stick with an Exponential$(1)$ prior for the two variance parameters since we don't know enough to say more about them. We can plug all of this in to `rstanarm::stan_lmer` as follows.

```{r, warning = F, message=F, results='hide', cache = T}
bayes_normal_reg <- stan_lmer(
  DRYBIO_AG_TPA_live_ADJ ~ tcc + (1 | ECOSUBCD),
  data = nonzero_train,
  prior = normal(2, 1),
  prior_intercept = normal(100, 40),
  prior_aux = exponential(1),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 2000, seed = 84735,
  cores = parallel::detectCores()
)
```


#### GLM 2: Gamma with log link

We'll stick with the same priors for the fixed effects as before except we'll put all of the parameter values on a log scale due to the fact that we're using a log link function.

- fixed effect slope: $\mathcal{N}(0.7, 0.1^2)$
- fixed effect intercept: $\mathcal{N}(4.6, 3.6^2)$

Similarly we'll keep our weakly informative prior for the variance of the random intercepts, and so all that's left is to decide what prior we should use for the shape parameter of the Gamma distribution. We'll follow the logic employed in Section \@ref(info) to do this. The estimation simply relies on the fact that the mean of a Gamma$(\alpha, \beta)$ distribution is $\alpha/\beta$ and the variance is $\alpha/\beta^2$. Here $\bar{y}$ and $s^2$ represent the sample mean and variance of Dry Above Ground Biomass from Live Trees.


\begin{equation}
\bar{y} = 80.45 \approx \frac{\alpha}{\beta}\qquad \text{and} \qquad s^2 = 6029 \approx \frac{\alpha}{\beta^2}
(\#eq:first)
\end{equation}

but now,

\begin{equation}
\frac{\bar{y}}{s^2} = 0.013 \approx \frac{\alpha / \beta}{\alpha / \beta^2} = \beta 
\end{equation}

and if we put our estimate for $\beta$ back into \@ref(eq:first) to get

\begin{equation}
\frac{\alpha}{0.013} \approx 80.45 \implies \alpha \approx 1.04
\end{equation}

We aren't positive about this guess but we know that the actuall parameter has to be non-negative so we'll use a Half-Cauchy$(1, 1)$ prior here. Note that `rstanarm::stan_glmer` assumes all `prior_aux` parameters are positive so we don't have to specify that it's a bounded Cauchy distribution.

```{r, warning = F, message=F, results='hide', cache = T}
bayes_gamma_reg <- stan_glmer(
  DRYBIO_AG_TPA_live_ADJ ~ tcc + (1 | ECOSUBCD),
  data = nonzero_train,
  family = Gamma(link = "log"),
  prior = normal(0.7, 0.1),
  prior_intercept = normal(4.6, 3.6),
  prior_aux = cauchy(1, 1),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 2000, seed = 84735,
  cores = parallel::detectCores()
)
```


Again, just from the length of these Bayesian model building sections compared to the Frequentist sections, it's clear how much extra work is required if you want to use informative priors. That being said, I hope it's clear how going through this process does force you to understand the dynamics of the data and know how to connect them back to the model, which in my mind is a very positive thing.

### Using zinf

In order to turn our individual models into a zero-inflation model we simply feed them into the appropriate `zinf_*` function.

```{r, warning= F, message=F}
frequentist_zi_mod <- zinf_freq(freq_reg, freq_log_reg)
bayesian_zi_mod_normal <- zinf_bayes(bayes_normal_reg, bayes_log_reg)
bayesian_zi_mod_gamma <- zinf_bayes(bayes_gamma_reg, bayes_log_reg)
```

If we examine the class of these objects we'll find that they're of class `zinf_freq` and `zinf_bayes` respectively.

```{r}
class(frequentist_zi_mod)
class(bayesian_zi_mod_normal)
```


At first this might not seem like much, but the real usefulness of the software package lies in the fact that it defines `zinf_bayes` and `zinf_freq` as S3 generics. In R "the job of an S3 generic is to perform method dispatch, i.e. find the specific implementation for a class."^[@wickham2019advanced]. So what `zinf` is really doing is defining methods like `predict()` and `summary()` for these zero-inflation model objects. The upshot is that we can now use these model objects much in the same way as we do for typical model objects in R.

And as we can see below, making predictions on the test dataset using our Frequentist zero-inflation model is a very familiar process. Since the estimand we're interested in is the Ecosubsection level mean and our test data set included two Ecosubsections, we get an output with two rows.


```{r}
predict(frequentist_zi_mod, newdata = test)
```

While the output will look slightly different for the Bayesian model, the process of getting the predictions is exactly the same. We'll start by looking at the Bayesian model where the GLM component assumes a Normal response

```{r, message=F, warning=F, cache=T}
bayes_preds_normal <- predict(bayesian_zi_mod_normal, newdata = test)
```

The object returned by running `predict` on one of the Bayesian zero-inflation models is a list that contains two items:

- A data frame containing the group level predictions which in this case are the centers of the posterior predictive distributions for each group
- A data frame containing the raw posterior predictive distributions for each group


If we examine the first item we see a very similar result to that of the direct output of the Frequentist predictions, except now we also have upper and lower bounds for 95% prediction intervals.

```{r}
bayes_preds_normal$posterior_predictive_centers
```

But since Bayesian predictions are distributions themselves, you can access the raw distributions as follows. We'll just look at the first five rows of this data frame to get an idea of what it looks like.

```{r, message=F, warning=F}
bayes_preds_normal$posterior_predictive_distribution %>% 
  head(5) 
```

With this we can actually plot the posterior predictive distributions for each group.

```{r, fig.align='center', out.width='85%', fig.cap="Posterior Predictive Distributions for the two groups in our test dataset"}
bayes_preds_normal$posterior_predictive_distribution %>% 
  ggplot(aes(x = x, fill = group)) +
  geom_density(alpha = 0.5, color = NA) +
  theme_bw()
```

Finally if we also compute the predictions for our Bayesian model with a Gamma GLM, then we can aggregate and look at how the models compared.

```{r, echo=F}
bayes_preds_gamma <- predict(bayesian_zi_mod_gamma, newdata = test)
```

```{r, message = F, warning=F, echo=F}
bayes_gam <- bayes_preds_gamma$posterior_predictive_centers %>% 
  rename(bayes_gamma = post_pred_centers) %>% 
  select(-c(lower, upper))

bayes_norm <- bayes_preds_normal$posterior_predictive_centers %>% 
  rename(bayes_normal = post_pred_centers) %>% 
  select(-c(lower, upper))

freq_norm <- predict(frequentist_zi_mod, newdata = test) %>% 
  rename(frequentist = prediction,
         group = ECOSUBCD)

true <- test %>% 
  group_by(ECOSUBCD) %>% 
  summarise(true_mean = mean(DRYBIO_AG_TPA_live_ADJ)) %>% 
  select(true_mean) %>% 
  pull()
  

left_join(bayes_gam, bayes_norm) %>% 
  left_join(freq_norm) %>% 
  mutate(true_mean = true) %>% 
  gt() %>% 
  cols_align(align = "center") %>% 
  fmt_number(
    columns = -group,
    decimals = 2
  ) %>% 
  cols_label(
    bayes_gamma = "Bayes Gamma",
    bayes_normal = "Bayes Normal",
    frequentist = "Frequentist",
    true_mean = "True Mean"
  )
```

It's nice to see that the Bayesian model where the non-zero response was modeled using a Gamma distribution performed the best, but again this is just an example about how to use the package and we shouldn't read too much into these short results.

## Next Steps

Right now the package is fairly limited in that it can primarily only deal with the models that we worked with in this thesis, so there's plenty of work to be done in that regard. Moreover, while zero-inflated data are fairly common,  zero-inflated clustered data are much less common, so a good extension would be to extend the package to work with models without random effects. That being said, I still believe that the package is enormously useful as is. In particular Bayesian prediction is an incredibly cumbersome task to do on your own, and it's even more difficult in this two-part model setting. Thus, having a package like `zinf` that will abstract away a lot of that messy process contributes a lot to the field of research surrounding these zero-inflation models.