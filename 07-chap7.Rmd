# Package Vignette {#vignette}

## Forestry Data

```{r, message=F, warning=F, echo= F}
library(here)
library(tidyverse)
library(gt)
data <- readRDS(here("data", "wa_plots_public.rds"))
```

In order to show off the functionality of the package that we wrote, we'll apply it to the Forestry data that we introduced all the way back in Chapter \@ref(intro-section). Recall that this forestry data is both zero-inflated and clustered making it a perfect setting for us to employ our models.

The particular subset of Forestry data that we'll be looking at comes from the State of Washington and contains `r nrow(data)` observations across 49 Eco-Subsections (which define our clusters). The summary statistics of the number of observations per group are as follows:

```{r, message=F, warning=F, echo=F}
summary(data %>% count(ECOSUBCD) %>% pull(n)) 
```

The covariate that we will be using from the forestry data is called "Tree Canopy Cover" (tcc) which is defined as the proportion of forest floor covered by a vertical projection of the tree crowns ^[@toney2008stem]. This is one of the FIA's remote sensed variables, so the information is associated with the pixels that overlay the individual plot.

The response variable that we will be using is called "Dry Above Ground Biomass from Live Trees" and is expressed in the units of per area density. This Dry Above Ground Biomass variable is one that is very important for understanding the health of forest ecosystems and plays a role in decisions regarding merchantable timber and fire hazard. For these reasons its vitally important for the FIA to be able to produce accurate estimates of this variable. 

The zero-inflation models presented in this thesis provide a reasonable approach for trying to do exactly that. That being said, no matter how good a method is, it can't be used if the tooling doesn't exist to actually perform it. Thus, the last portion of this thesis will be devoted to describing the very early stages of an R software package that we have written to build and utilize these zero-inflation models.


## R Package: zinf

We start by installing the developmental version of the package and loading it into our R session.

```{r, message=F, warning=F}
devtools::install_github("joshyam-k/zinf")
library(zinf)
```


The two main functions that `zinf` contains are `zinf_bayes` and `zinf_freq` which as the names suggest relate to the Frequentist and Bayesian versions of the zero-inflation models.

The challenge of writing a software package for these zero-inflation models was truthfully a lot trickier than I initially thought it would be. There were two different possible structural versions that I considered to be viable:

1. Write a function that takes as argument all the individual pieces needed to build each of the two models and internally build the two models and then define all of the neccessary functionality.
2. Write a function that takes as argument each of the two already built models and internally combine the models into a single object and define all of the neccessary functionality.

On the one hand, version (1) is nice in that it "does it all" for the user. But the main problem with version (1) is that the functions `lme4::lmer` and `lme4::glmer` for Frequentist models and `rstanarm::stan_lmer` and `rstanarm::stan_glmer` for Bayesian models are already so popular and well developed and robust that it seems silly to to rely on that pre-existing software. After all, as we showed in Section \@ref(sepsim), the Bayesian models can be built separately and made as complex desired, and this is also certainly the case for the Frequentist models. So, while there's an argument to be made for the abstraction of all model building processes, the reality is that under version (1) if I wanted my package to be as familiar and robust as the pre-existing software for building the individual models, I would have to essentially take as arguments every possible function arguments that those `lme4` and `rstanarm` functions take, and internally feed them back into those functions. At that point the question becomes: why not just have the user build the individual models separately and then feed those finished models into our function? And that lands us directly at version (2).

```{r tbl2, echo=F, warning=F, message=F, fig.cap="Two Software Versions", out.width="90%", fig.align="center"}
include_graphics(path = "figure/v1v2.png")
```


In the Frequentist setting, version (1) doesn't seem that bad, especially since most of the time a user only specifies the model formula and the data set within the `lme4` functions, but things grow a lot worse in the bayesian versions where all kinds of priors and MCMC settings need to be specified. In the end, I decided that it made more sense to build version (2). While this does leave more work for the user than I would have liked, I think that the `zinf` functions still have enormous usefullness in that they define methods for producing a summary of the zero-inflation model and most importantly for predicting with the zero-inflation model.


## Application

The only data preparation we need to do is to create a data set that contains all of the data points and a data set that only contains data points with a non-zero response value.

```{r}
full_data <- data
nonzero_data <- data %>% 
  filter(DRYBIO_AG_TPA_live_ADJ > 0)
```


### Individual Frequentist models

We'll start by building the individual Frequentist models.

```{r, message=F, warning = F}
library(lme4)

freq_model1 <- lmer(
  DRYBIO_AG_TPA_live_ADJ ~ tcc + (1 | ECOSUBCD),
  data = nonzero_data
  )

freq_model2 <- glmer(
  DRYBIO_AG_TPA_live_ADJ != 0 ~ tcc + (1 | ECOSUBCD),
  family = binomial,
  data = full_data
  )
```


The key characteristics of these models are the specification of the covariates and the response, the random intercept term for the Ecosubsection (ECOSUBCD), and the use of the non-zero data set to fit the first model.

### Individual bayesian models

#### Logistic Regression Model

Next we'll build the individual Bayesian models. We'll start with the logistic regression model that requires priors for 

- The fixed effect slope (for tcc)
- The fixed effect intercept
- The variance of the random intercepts

The prior fixed effect slope should reflect our vague sense for how the probability of Dry Above Ground Biomass from Live Trees being non-zero changes with Tree Canopy Cover. With limited knowledge of how forest attributes interact we'll assume that the slope is positive and centered somewhere around 0.5, so we'll use a prior of $\mathcal{N}(0.5, 0.25^2)$.

The fixed effect intercept that `rstanarm::stan_glmer` asks for is actually the centered intercept. This can be conceptualized as our guess for the probability of being non-zero for an average plot. Given that the response variable is zero in around 40% of the plots in our data set we'll guess that this probability is around 0.4. We have to be careful here, since logistic regression employs a logit link, we actually want to set the prior mean to be on the log-odds scale, so we use $\log{(0.4/(1-0.4))} = -0.4$ as the center. We might guess that the range of the propbability of being non-zero for an average plot might range from 0.1 to 0.7 which translates to a log odds range of $(-2.2, 0.85)$. Thus the prior for the centered fixed effect intercept will thus be $\mathcal{N}(-0.4, 0.7^2)$.

Finally, the variance of the random intercepts is not really a parameter that we can use the data to get a great guess for, so we'll utilize a weakly informative prior of Exponential$(1)$. In `rstanarm::stan_glmer` this is written slightly differently using a function called `decov` but in this setting that is just the Exponential$(1)$ prior that we want.

We'll stick with the default MCMC settings and build the model as follows

```{r, warning = F, message=F, eval = F}
library(rstanarm)
bayes_model1 <- stan_glmer(
  DRYBIO_AG_TPA_live_ADJ > 0 ~ tcc + (1 | ECOSUBCD),
  data = full_data,
  family = binomial,
  prior = normal(0.5, 0.25),
  prior_intercept = normal(-0.4, 0.7^2),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 2000, seed = 84735
)
```


#### GLM 1: Normal with Identity link

We now turn to the two versions of the generalized linear model. We'll start with the model that uses a Normal distribution and the identity link- i.e Normal linear regression. This time the parameters that need priors are all the same as from the previous one, except now we also need a prior for the variance of the error term.

We can examine a scatter plot of Tree Canopy Cover on Dry Above Ground Biomass from Live Trees to choose our prior for the fixed effect slope. We can do some rise-over-run calculations to end up with a guess that the fixed effect slope is around 2 and probably doesn't go outside of the range (0.5, 5). Thus we'll use a $\mathcal{N}(2, 1^2)$ prior here.

```{r nzdata, echo=F, message=F, warning=F, fig.align='center', out.width='90%', fig.cap="Relationship between Tree Canopy Cover and Dry Above Ground Biomass from Live Trees"}
nonzero_data %>% 
  mutate(ratio = floor(DRYBIO_AG_TPA_live_ADJ/tcc)) %>% 
  filter(ratio > 0) %>% 
  mutate(DRYBIO_AG_TPA_live_ADJ = DRYBIO_AG_TPA_live_ADJ + rnorm(1851, 0, 20)) %>% 
  filter(DRYBIO_AG_TPA_live_ADJ > 0) %>% 
  ggplot(aes(x = tcc, y = DRYBIO_AG_TPA_live_ADJ)) +
  geom_point(alpha = 0.5, size = 2) +
  theme_bw() +
  labs(
    x = "Tree Canopy Cover",
    y = "Dry Above Ground Biomass from Live Trees"
  )
```


Again we can refer back to Figure \@ref(fig:nzdata) to get our guess for the the centered intercept. At an average value of tcc (say 50), the value of the response looks to be around 100, but this could plausibly range anywhere between 20 and 180, so we'll use a $\mathcal{N}(100, 40^2)$ prior for the centered intercept.

Finally we'll stick with an Exponential$(1)$ prior for the two variance parameters since we don't know enough to say more about them. We can plug all of this in to `rstanarm::stan_lmer` as follows.

```{r, eval = F}
bayes_model2 <- stan_lmer(
  DRYBIO_AG_TPA_live_ADJ ~ tcc + (1 | ECOSUBCD),
  data = nonzero_data,
  prior = normal(2, 1),
  prior_intercept = normal(100, 40),
  prior_aux = cauchy(1),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 2000, seed = 84735
)
```


#### GLM 2: Gamma with log link

We'll stick with the same priors for the fixed effects as before except we'll put all of the parameter values on a log scale due to the fact that we're using a log link function.

- fixed effect slope: $\mathcal{N}(0.7, 0.1^2)$
- fixed effect intercept: $\mathcal{N}(4.6, 3.6^2)$

Similarly we'll keep our weakly informative prior for the variance of the random intercepts, and so all that's left is to decide what prior we should use for the shape parameter of the Gamma distribution. We'll follow the logic employed in Section \@ref(info) to do this. The estimation simply relies on the fact that the mean of a Gamma$(\alpha, \beta)$ distribution is $\alpha/\beta$ and the variance is $\alpha/\beta^2$. Here $\bar{y}$ and $s^2$ represent the sample mean and variance of Dry Above Ground Biomass from Live Trees.


\begin{equation}
\bar{y} = 80.45 \approx \frac{\alpha}{\beta}\qquad \text{and} \qquad s^2 = 6029 \approx \frac{\alpha}{\beta^2}
(\#eq:first)
\end{equation}

but now,

\begin{equation}
\frac{\bar{y}}{s^2} = 0.013 \approx \frac{\alpha / \beta}{\alpha / \beta^2} = \beta 
\end{equation}

and if we put our estimate for $\beta$ back into \@ref(eq:first) to get

\begin{equation}
\frac{\alpha}{0.013} \approx 80.45 \implies \alpha \approx 1.04
\end{equation}

We aren't positive about this guess but we know that the actuall parameter has to be non-negative so we'll use a Half-Cauchy$(1, 1)$ prior here. Note that `rstanarm::stan_glmer` assumes all `prior_aux` parameters are positive so we don't have to specify that it's a bounded Cauchy distribution.

```{r, eval = F}
bayes_model3 <- stan_glmer(
  DRYBIO_AG_TPA_live_ADJ ~ tcc + (1 | ECOSUBCD),
  data = nonzero_data,
  family = Gamma(link = "log"),
  prior = normal(0.7, 0.1),
  prior_intercept = normal(4.6, 3.6),
  prior_aux = cauchy(1, 1),
  prior_covariance = decov(reg = 1, conc = 1, shape = 1, scale = 1),
  chains = 4, iter = 2000, seed = 84735
)
```


Again, just from the length of these Bayesian model building sections compared to the Frequentist sections, it's clear how much extra work is required if you want to use informative priors. That being said, I hope it's clear how going through this process does force you to understand the dynamics of the data and know how to connect them back to the model, which in my mind is a very positive thing.

### Using zinf

In order to turn our individual models into a zero-inflation model we simply feed them into the appropriate `zinf_*` function.

```{r, eval = F}
frequentist_zi_mod <- zinf_freq(freq_model1, freq_model2)
bayesian_zi_mod_normal <- zinf_bayes(bayes_model1, bayes_model2)
bayesian_zi_mod_gamma <- zinf_bayes(bayes_model1, bayes_model3)

```

If we examine the class of these objects we'll find that they're of class `zinf_freq` and `zinf_bayes` respectively. At first this might not seem like much, but the real usefulness of the software package lies in the fact that it defines methods for generic functions like `summary()` and `predict()` on these new zero-inflation model objects. In particular prediction can be an incredibly cumbersome task in Bayesian modeling, so there's enormous usefulness in abstracting away all of that work.

