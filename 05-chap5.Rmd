# Simulation Study {#sim-study}

```{r, echo = F, warning = F, message = F}
library(tidyverse)
library(here)
library(gt)
library(patchwork)
```

We now introduce the set up for the simulation study that will be the main avenue through which we evaluate our models. 

## Aims

The first aim for this simulation study is to understand when we might benefit from using a Bayesian model.

In order to try to answer this question of *when* we will be turning a few dials. Because we are functioning in a setting with grouped data we will vary the data along two axes

-   Number of groups
-   Number of observations per group

The second aim is to get a sense for the role that the prior plays in a Bayesian analysis. To do so we will vary the quality of the priors put on the model parameters. The two levels will be

-   Uninformative
-   Informative

The idea to vary the quality of the priors comes out of the idea that while the likelihood does dominate the prior in large sample size settings, the prior plays a much larger role in smaller sample settings and thus we might expect a lot more regularization and model convergence in the Bayesian models when more information is added through the priors.

## Data and Dials

### Estimands

The primary estimand for this simulation study will be $Y_j$: the mean of our response variable in group $j$. In order to stay consistent across all models and simulation runs, we only made predictions on group $1$.

### Data Generating Process

Our population data for the simulation was generated by the following process that takes as inputs the number of domains ($J$) and the number of total observations ($N$). The number of observations per group is thus ($n = N/J$). The population was generated using $N = 50\cdot 500$ and $J = 50$, to give us 500 observations per group.

The DGP is defined as follows

$$
\begin{aligned}
z_{ij} &\sim \text{Bernoulli}(p_{ij}) \ \ \ \ \text{where} \ \ \ \ p_{ij} = \frac{\text{exp}\big(1.5 + X_{ij} + v_j\big)}{1 + \text{exp}\big(1.5 + X_{ij} + v_j\big)} \\
y^*_{ij} &\sim \text{Gamma}(3, \  1/b_{ij}) \ \ \ \ \text{where} \ \ \ \ b_{ij} = 10 + X_{ij} + u_{j} \\
y_{ij} &=z_{ij}\cdot y^*_{ij}
\end{aligned}
$$

Where $X_{ij} \overset{\text{iid}}{\sim} \mathcal{N}(0, 1)$, $v_j \overset{\text{iid}}{\sim} \mathcal{N}(0, 1)$ and $u_{ij} \overset{\text{iid}}{\sim} \mathcal{N}(0, 1)$

This data generating process checks all of our boxes as it produces zero-inflated data with a grouped structure and a moderate relationship between the response $Y$ and a covariate $X$. For example in Figure \@ref(fig:simdataplot) we examine the generated population data and see that the response variable is indeed zero inflated with a strictly positive and continuous non-zero portion.

```{r simdataplot, warning = F, message = F, echo=F, fig.align='center', out.width='80%', fig.cap='The distribution of the response variable in the population that we generated for our simulation.'}
DGP <- function(N, g) {
  
  # Generate X
  X <- rnorm(N, 0, 1)
  
  # observations per group
  n <- N/g
  group <- rep(seq(1, g), n) 
    
  # Generate REs for each model individually
  random_effects_y <- rnorm(g, 0, 1) 
  random_effects_z <- rnorm(g, 0, 1) 
    
  # 1.5 chosen so that we have <50% 0'z
  xb <- 1.5 + X + random_effects_z[group]
  
  # Probability of being non-zero
  prob <- exp(xb) / (1 + exp(xb)) 
  
  # simulating Z
  Z <- 1*(runif(N) < prob) 
    
  # Make Y
  ypure <- 10 + X + random_effects_y[group] 
    
  # generate gamma distributed Y
  Y <- rgamma(N, shape = 3, rate = 1/ypure) 
  
  # zero-inflate data by multiplying by Z
  Y <- Z*Y 
  
  return(tibble(X, Y, Z, group))
}
set.seed(16)
data <- DGP(50*500, 50)


ggplot(data, aes(x = Y)) +
  geom_histogram(
    fill = "#023047",
    color = "white",
    alpha = 0.9
  ) +
  theme_bw()
```

If we examine the correlation between $X$ and $Y$ we get `r round(cor(data$X, data$Y), 3 )` for the entire data set. And indeed when we examine the plot of $Y$ against $X$ we do see a relationship. Here we just plot points from a single group so as not to overclutter the plot.

```{r, warning = F, message = F, echo=F, fig.align='center', out.width='80%', fig.cap='Scatterplot of the main covariate against the response variable in Group 1 of our generated population data.'}
data %>% 
  filter(group == 1) %>% 
  ggplot(aes(X, Y)) +
  geom_point(size = 3, alpha = 0.6) +
  theme_bw()
```


### Dials

The exact levels of the dials for the number of groups and number of observations per group will be

-   n = $\{15, 30, 50\}$
-   J = $\{5, 10, 25, 50\}$

Note that these levels reflect the number of **sampled** groups and observations per group. We ran a simulation for each combination of those levels, for a total of 12 runs, to get a full sense of how the models perform across each of these measures of how "small" the data is. In order to generate data sets for these settings we first sampled $J$ groups from the population, and then within those groups we sampled $n$ observations. Importantly, because we were always predicting on group 1, we always forced that group into each simulation iteration. For each of the 12 individual settings we ran 400 total iterations of the simulation.

Next, within each $(n,\ J)$ combination we built the Bayesian models with two types of priors: Non-Informative and Informative. Note that across our models we have a variety of parameters that require priors.

-   For the Normal regression model we have the the fixed effects coefficients $(\boldsymbol{\beta})$, the variance parameter for the random intercepts $(\sigma_u^2)$, and the variance parameter for the observation level model error $(\sigma_{\varepsilon}^2)$.

-   For the Gamma regression model we once again have the fixed effects coefficients $(\boldsymbol{\beta})$, and the variance parameter for the random intercepts $(\sigma_u^2)$, but this time we also have the shape parameter $(\alpha)$.

-   For the Logistic regression model we have new fixed effects coefficients $(\boldsymbol{\gamma})$ and the variance parameter for the random intercepts $(\sigma_v^2)$.

Note that across all models the random effects are assumed to be $\mathcal{N}(0, \tau^2)$ distributed and thus can be thought of as universally having a normal prior with a variance hyperparameter. While the distributions chosen for these priors were introduced in the methods section, we will now choose real valued numbers used to center and scale those priors.

The parameters that require priors can be placed into 3 main groups: fixed effects, variance, and shape. We simplified the process of assigning the levels of priors by not varying the priors within these groups within each bracket of quality.

#### Non-Informative

In order to include no prior knowledge in your analysis, a flat prior can be used: $p(\theta) \propto 1$. Recall that the posterior can be factored into a normalized product of the likelihood and the prior. Thus, to use a flat, non-informative, prior is to "let the data speak for itself" and utilize only the likelihood function in estimating the posterior. That being said prior distributions should still not cover parameter values that are impossible, and thus the flat priors will have supports that are restricted to the range of possible values that the parameter could take on. For the fixed effects parameters a support of $(-100, 100)$ was used and for the variance and shape parameters $(0, 100)$ was used.

```{r flatpriors, echo = F, warning=F, message=F, fig.cap = 'Plots showing what flat prior distributions look like for each parameter.'}
fixed_eff <- ggplot(data.frame(x = c(-100, 100)), aes(x)) +
  xlim(-100, 100) + 
  ylim(0, 1/(100)) +
  stat_function(
    fun = dunif, args = list(min = -100, max = 100),
    geom = "area", fill = "#dce4e8", alpha = 0.9
    ) + 
  stat_function(
    fun = dunif, args = list(min = -100, max = 100),
    linewidth = 1
    ) +
  theme_bw() +
  labs(
    x = "X",
    y = "Density",
    title = "Fixed Effects"
  )
  

variance <- ggplot(data.frame(x = c(0, 100)), aes(x)) +
  xlim(0, 100) +
  ylim(0, 1/(100)) +
  stat_function(
    fun = dunif, args = list(min = 0, max = 100),
    geom = "area", fill = "#dce4e8", alpha = 0.9
    ) + 
  stat_function(
    fun = dunif, args = list(min = 0, max = 100),
    linewidth = 1
    ) +
  theme_bw() +
  labs(
    x = "X",
    y = "",
    title = "Variance"
  )

shape <- variance +
  labs(title = "Shape")
```

```{r, warning = F, message = F, echo=F, fig.align='center', out.width='80%'}
fixed_eff + variance + shape
```

#### Informative {#info}

Next, the goal with our informative priors was to add more information by choosing to center and scale the prior distributions with the observed data in mind. Naturally, since we know the exact process that generated the data it might be tempting to center the priors exactly on the correct parameter values, but to do so would certainly be both unfair and unrealistic. Instead exploratory data analysis was utilized to make the decisions.

An examination of a scatterplot of the response and covariates leads us to believe that the fixed effects coefficients for the models fitting the non-zero portion of the response are small *positive* numbers between the range of 0 to 10 and thus the prior $\mathcal{N}(5, 3^2)$ was used.

```{r, warning = F, message = F, echo=F, fig.align='center', out.width='80%'}
ggplot(data.frame(x = c(-10, 20)), aes(x)) +
  stat_function(
    fun = dnorm, args = list(mean = 5, sd = 3),
    geom = "area", fill = "#a4b4bd", alpha = 0.7
  ) +
  stat_function(
    fun = dnorm, args = list(mean = 5, sd = 3),
    linewidth = 1
  ) +
  theme_minimal()
```

On the other hand, we tweaked the scale of this prior for the logistic regression model since the response being modeled is on a different scale. At this point we don't have much of an idea of how the chance of being non-zero changes as the covariate changes, so we use a $\mathcal{N}(1,1)$ prior for the fixed effects coefficients in the logistic regression model (this may seem like a tight distribution, but in the context of logistic regression slope coefficients it's really quite wide).

Next, following the advice of [@gelman2006prior] the scale parameter of $2.5$ was used to parametrize the Cauchy prior put on all variance parameters. In full the $\text{Half-Cauchy}(0, 2.5)$ prior was utilized.

```{r, warning = F, message = F, echo=F, fig.align='center', out.width='80%'}
ggplot(data.frame(x = c(0, 20)), aes(x)) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 2.5),
    geom = "area", fill = "#a4b4bd", alpha = 0.7
  ) +
  stat_function(
    fun = dcauchy, args = list(location = 0, scale = 2.5),
    linewidth = 1
  ) +
  theme_minimal() +
  labs(
    x = "X",
    y = "Density"
  )
```

Finally, since the non-zero portion of the response variable has a mean of `r round(mean(data[data$Z == 1, ]$Y), 2)`, we can use the following "back of the napkin" calculation to get an idea for what the shape parameter should be centered on. Recall that a $\text{Gamma}(\alpha, \beta)$ distribution has mean $\alpha / \beta$ and variance $\alpha / \beta^2$. Thus we can say that,

$$
\bar{y} = 31.4 \approx \frac{\alpha}{\beta}\qquad \text{and} \qquad s^2 = 338.8 \approx \frac{\alpha}{\beta^2}
$$

But some algebra tells us then that

$$
\frac{\bar{y}}{s^2} = 0.09 \approx \frac{\alpha / \beta}{\alpha / \beta^2} = \beta 
$$

but now plugging in our approximate value for $\beta$ gives us

$$
\frac{\alpha}{0.09} \approx 31.4 \implies \alpha \approx 2.8
$$

And so we might guess that the shape parameter is around 3. Obviously this calculation will vary quite a bit depending on the sample that we generate, so we use the scale parameter of 5 to emphasize our uncertainty in that estimate. Thus the complete prior used was $\text{Half-Cauchy}(3, 5)$.

```{r, warning = F, message = F, echo=F, fig.align='center', out.width='80%'}
ggplot(data.frame(x = c(0, 30)), aes(x)) +
  stat_function(
    fun = dcauchy, args = list(location = 3, scale =5),
    geom = "area", fill = "#a4b4bd", alpha = 0.7
  ) +
  stat_function(
    fun = dcauchy, args = list(location = 3, scale = 5),
    linewidth = 1
  ) +
  theme_minimal()
```



## Methods

There will be 3 different models run throughout the simulation.

-   Frequentist two-part:
    -   Linear Random Effects Model (`lme4::lmer`)
    -   Logistic Regression Random Effects Model (`lme4::glmer`)
-   Bayesian two-part #1
    -   Bayesian Linear Random Effects Model (`rstanarm::stan_lmer`)
    -   Bayesian Logistic Regression Random Effects Model (`rstanarm::stan_glmer`)
-   Bayesian two-part #2
    -   Bayesian Gamma Regression Random Effects Model (`rstanarm::stan_glmer`)
    -   Bayesian Logistic Regression Random Effects Model (`rstanarm::stan_glmer`)

## Performance metrics {#metrics}

As mentioned previously, due to the fact that our data has a clustered structure, we chose to evaluate all of our models on a single group. Thus all of the following performance metrics were computed for the same individual group (group 1) which had a true mean value of $Y_1 = 23.7$

The most broadly used performance metric of an estimator is the Mean Squared Error (MSE). We use the Root Mean Squared Error (RMSE) which is just a rescaling of this metric and is computed as follows:

$$
\text{RMSE}_j  = \sqrt{\frac{1}{S}\sum_{s = 1}^{S}\bigg(\hat{Y}_{j, s} - Y_{j}\bigg)^2}
$$

Here $S$ is the total number of simulation reps, $\hat{Y}_{j, s}$ is the estimated mean of the response variable in group $j$ for simulation rep $s$, and $Y_{j}$ is the true mean of the response variable in group $j$. Since we get this true value from our generated population, it does not vary by simulation rep.

Next we will be examining the Empirical Bias of each model:

$$
\text{Empirical Bias}_j = E[\hat{Y}_j] - Y_{j} \qquad \text{where} \qquad E[\hat{\mu}_j] = \frac{1}{S}\sum_{s = 1}^S\hat{Y}_{j, s}
$$

As well as the Empirical Variance of each model:

$$
\text{Empirical Var}_j  = \frac{1}{(S-1)}\sum_{s = 1}^S\Big(\hat{Y}_{j,s} - E[\hat{Y}_j]\Big)^2
$$

We will look at Predictive Interval (PI) coverage at a 95% confidence level for the Bayesian Models:

$$
\text{Coverage}_j = \frac{1}{S}\sum_{s = 1}^S \mathbb{I}(Y_{j} \in \text{PI}(\hat{Y}_{j, s}))
$$

And finally we will look at the total number of model failures.

-   For the Frequentist models, a model failure was registered whenever one of the "Model failed to converge" warnings from `lme4` was returned, although even in small sample sizes our data was robust enough to elicit zero model failures.
-   For the Bayesian models, a model failure was registered whenever the MCMC algorithm failed one or more of the diagnostics that indicate that the algorithm has converged and is stable. The Stan programming language is robust enough that you should never trust the results when one of these warnings are administered, and so while potentially overly harsh in some cases, we chose to count any convergence warning as a model failure.

Importantly, one solution to combating MCMC convergence issues is to bump up the number of iterations that the algorithm progresses through. That being said, this is not a sure fire solution, and always comes at the cost of longer computation time. The default number of iterations is 2000 and is considered a reasonable number of iterations for most models. Because the models we are fitting are fairly complex, ran a few tests to guage how many iterations we should use in order to try to minimize model failures while keeping model run-times reasonable. We saw good improvements up to 5000 iterations and then diminishing returns afterwards, so we used 5000 iterations for all of our Bayesian models.

